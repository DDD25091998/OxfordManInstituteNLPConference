{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ae4cd8",
   "metadata": {},
   "source": [
    "# Oxford Man Institute NLP Tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d7c68",
   "metadata": {},
   "source": [
    "## 1. Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e5b5a0",
   "metadata": {},
   "source": [
    "There are several ways of performing sentiment classification on a document or article, ranging from word-counts to modern Transformer-based Language Models. In this tutorial we will take you through a range of classification techniques:\n",
    "- Loughran & McDonald financial sentiment dictionary\n",
    "- Naive Bayes Classifier\n",
    "- BERT out of the box\n",
    "- BERT fine-tuned on general sentiment datasets\n",
    "- FinBERT \n",
    "    - BERT that has been trained on positive and negative financial documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4029a21a-8149-4aa9-b726-2064e745b9ef",
   "metadata": {},
   "source": [
    "## 2. Load and analyse the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272fa01e",
   "metadata": {},
   "source": [
    "### Import packages and load dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e8c3d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import inspect\n",
    "import csv\n",
    "import operator\n",
    "from tutorial_tools import mutual_information\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c00663a-8d6a-4ea3-8b10-680d917b5183",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d17b5ac4-3480-44c0-9083-4b9e4780e34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\AppData\\Local\\Temp\\ipykernel_17576\\3544662773.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv('data\\\\FinancialPhraseBank-v1.0\\\\Sentences_AllAgree.txt',\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In the third quarter of 2010 , net sales incre...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Operating profit rose to EUR 13.1 mn from EUR ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Operating profit totalled EUR 21.1 mn , up fro...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     class\n",
       "0  According to Gran , the company has no plans t...   neutral\n",
       "1  For the last quarter of 2010 , Componenta 's n...  positive\n",
       "2  In the third quarter of 2010 , net sales incre...  positive\n",
       "3  Operating profit rose to EUR 13.1 mn from EUR ...  positive\n",
       "4  Operating profit totalled EUR 21.1 mn , up fro...  positive"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data\\\\FinancialPhraseBank-v1.0\\\\Sentences_AllAgree.txt',\n",
    "            encoding = 'ISO-8859-1',on_bad_lines='skip',sep = '.@', names=['text', 'class'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac12bc77-18c8-400c-9ff8-cefcc2148b19",
   "metadata": {},
   "source": [
    "#### Data formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26553ef6-a6e1-4dec-ba10-604704c86e41",
   "metadata": {},
   "source": [
    "The dataset we are using has already been cleaned as it is widely used in the field. If you are using your own custom dataset it may be necessary to clean the text of special characters, URLs, user mentions, emojis etc. \n",
    "\n",
    "We will add another columns to our dataset that contains a **tokenised version** of the text as this will form the basis of our Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e593fd5-367f-4b2b-8f88-b63858090acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "df[\"tokenised_text\"] = df.text.apply(lambda x: token_pattern.findall(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54c1c79-7c2e-46c0-8e0b-88a30da01aef",
   "metadata": {},
   "source": [
    "**Remove the neutral category** so that we are left with a more extreme examples in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f2632802-7f69-4b93-ab2f-d412eaaa7a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'positive']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['class'] != 'neutral']\n",
    "categories = list(set(df['class']))\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "448becfe-ea9c-475a-b61d-3e5c19b4c97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "873 entries in the dataset\n",
      "303 entries / 34.71% labelled as negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_negative = df[df['class']==\"negative\"].shape[0]\n",
    "prop_negative = df[df['class']==\"negative\"].shape[0]/df.shape[0]\n",
    "\n",
    "print(f\"{df.shape[0]} entries in the dataset\")\n",
    "print(\"{} entries / {:.2%} labelled as negative\\n\".format(n_negative, prop_negative)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89c750f-eee1-40a9-b6af-812ee7a1411b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### What kind of text are we dealing with?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4656f863-165f-4af2-8d23-bbcc47ff1ca5",
   "metadata": {},
   "source": [
    "The Financial PhraseBank was used by Maks and Vossen (2014) in their paper ***\"Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts\"***. The authors scraped the LexisNexis database for news articles about companies on the OMX Helsinki database.\n",
    "\n",
    "Although very small, the dataset has since been widely used in the financial community to test and deploy domain specific language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fa191de-e03f-48f7-a236-dcbd19b8ed94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest negative sentence is: \n",
      "\n",
      " 2116    Finnish Exel Composites , a technology company that designs , manufactures , and markets composite profiles and tubes for various industrial applications , reports its net sales decreased by 0.6 % in the second quarter of 2010 to EUR 19.2 mn from EUR 19.3 mn in the corresponding period in 2009 \n",
      "Name: text, dtype: object \n",
      "\n",
      "\n",
      "The longest positive sentence is: \n",
      "\n",
      " 831    MANAVIGATOR-September 7 , 2010-Kemira unveils Indian JV with IVRCL Finnish chemicals group Kemira ( HEL : KRA1V ) on Tuesday announced it has inked a deal to form a joint venture in India with local construction firm IVRCL Infrastructure and Projects Ltd ( BOM :530773 ) \n",
      "Name: text, dtype: object \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cata in categories:\n",
    "    cata_df = df[df['class'] == cata]\n",
    "    cata_lens = cata_df['text'].str.len()\n",
    "    max_sentence = cata_df[cata_lens == cata_lens.max()]\n",
    "    with pd.option_context('display.max_colwidth', 500):\n",
    "        print(f'The longest {cata} sentence is: \\n\\n', max_sentence['text'], '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a333bc-6838-4845-ab02-85018db8497a",
   "metadata": {},
   "source": [
    "#### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee3dd591-1144-4487-aa06-44255170abc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, dev_text, test_text = dict(), dict(), dict()\n",
    "\n",
    "for c_i in categories:\n",
    "    train_text[c_i], devtest_text = train_test_split(df[df['class']==c_i].tokenised_text,\n",
    "                                                         train_size=0.7, random_state=123)\n",
    "    dev_text[c_i], test_text[c_i] = train_test_split(devtest_text,\n",
    "                                                         train_size=0.5, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a76f108-3939-4549-946b-d5bc2bc5ba9f",
   "metadata": {},
   "source": [
    "Create vocab dict and remove stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d1ecb88-57f6-412d-9fe9-9308b163826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocab dict\n",
    "vocab = defaultdict(Counter)\n",
    "n_text = defaultdict(Counter)\n",
    "\n",
    "# Create vocabularies\n",
    "for c_i in categories:\n",
    "    for text in train_text[c_i]:\n",
    "        vocab[c_i].update(text)\n",
    "        n_text[c_i].update(set(text))\n",
    "\n",
    "# Remove stopwords\n",
    "stopwords = list()\n",
    "\n",
    "with open('./data/stopwords.txt', 'r') as fd:\n",
    "    reader = csv.reader(fd)\n",
    "    for row in fd:\n",
    "        stopwords.append(row.replace(\"\\n\",\"\"))\n",
    "        \n",
    "for c_i in categories:\n",
    "    for sw in stopwords:\n",
    "        del vocab[c_i][sw]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d11b66-cc21-4567-a8be-09730acb92b1",
   "metadata": {},
   "source": [
    "#### Inspection of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c6e436c-3452-44f7-be91-79bc1deb8eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in NEGATIVE class:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('EUR', 174),\n",
       " ('mn', 125),\n",
       " ('profit', 83),\n",
       " ('year', 57),\n",
       " ('net', 52),\n",
       " ('quarter', 51),\n",
       " ('mln', 50),\n",
       " ('sales', 50),\n",
       " ('2008', 44),\n",
       " ('million', 43)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most common words in POSITIVE class:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('EUR', 237),\n",
       " ('mn', 157),\n",
       " ('profit', 114),\n",
       " ('year', 106),\n",
       " ('net', 103),\n",
       " ('period', 88),\n",
       " ('million', 82),\n",
       " ('sales', 80),\n",
       " ('Finnish', 75),\n",
       " ('mln', 71)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for c_i in categories:\n",
    "    print(f\"Most common words in {c_i.upper()} class:\")\n",
    "    display(vocab[c_i].most_common(10))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb892344-e0b9-4e98-930a-f6e32dbdc065",
   "metadata": {},
   "source": [
    "#### Mututal information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8a374c-4730-4ca6-9a5e-5ad9bb94132f",
   "metadata": {},
   "source": [
    "Mutual information can help us explain the differences between word distributions. Understanding the features that differentiate a certain category from another can prove very useful for interpretting differences between categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4d80aac-6ce1-4b2f-b2da-374ea13876f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of articles in each class, which we will need to calculate mutual information\n",
    "n_total = dict()\n",
    "for c_i in categories:\n",
    "    n_total[c_i] = len(train_text[c_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00532125-9f5e-4d78-b818-ccec005d1622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 overall most informative features:\n",
      "\n",
      " ['fell', 'increased', 'half', '2008', 'nine', 'due', 'EUR', '17', '2007', 'mn', 'quarter', 'eur', 'profit', 'Helsinki', 'first', 'trade', 'prices', 'electronics', '2010', 'excluding', 'items', 'reports', 'negative', 'third', 'order']\n"
     ]
    }
   ],
   "source": [
    "mi_list = sorted([(mutual_information(w, n_text, n_total), w) for w in set(vocab['negative']).union(set(vocab['positive']))], reverse=True)\n",
    "\n",
    "print('25 overall most informative features:\\n\\n', [w for mi, w in mi_list][:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0879d0bc",
   "metadata": {},
   "source": [
    "## 3. Traditional sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d35271",
   "metadata": {},
   "source": [
    "### Loughran & McDonald classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5446979d-60e4-4d7a-9597-d608a36170a0",
   "metadata": {},
   "source": [
    "Loughran & McDonald released their master dictionary in 2011 in conjunction with their paper “When is a Liability not a Liability? Textual Analysis, Dictionaries, and 10-Ks\". The dictionary lists a number of words and includes negative, positive, uncertainty, litigious, strong modal, weak modal, and constraining tags. \n",
    "\n",
    "There are several shortcomings to this simplistic approach:\n",
    "\n",
    "- **Some words don't appear in the dictionary (fall, rise, etc.)**\n",
    "- **Some words are negative/positive given the context they are written (profit, expenditure, etc.)**\n",
    "- **Simple counts of words don't necessarily infer the overall sentiment**\n",
    "    - *Hatred for football has always confused me; there are so many haters who attack the sport, but I have always loved it.* - 3 negative words and 1 positive word.\n",
    "\n",
    "\n",
    "We have taken the words that have a negative and positive tag for our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "608eae79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some examples of negative words:  ['abandon', 'abandoned', 'abandoning', 'abandonment', 'abandonments']\n",
      "Some examples of positive words:  ['able', 'abundance', 'abundant', 'acclaimed', 'accomplish']\n"
     ]
    }
   ],
   "source": [
    "lmdict = np.load('data/LoughranMcDonald_dict.npy', allow_pickle='TRUE').item()\n",
    "print('Some examples of negative words: ', lmdict['Negative'][:5])\n",
    "print('Some examples of positive words: ', lmdict['Positive'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "441f95e2-be4a-4263-96da-9dc07d68a45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able', 'abundance', 'abundant', 'acclaimed', 'accomplish']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmdict['Positive'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00495202-583a-4b11-8ede-7cfb61fd41db",
   "metadata": {},
   "source": [
    "Check to see if a word appears in the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "004cd19d-8fbe-4e5c-9538-2cfc0017d73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, fall is not in the Loughran & McDonald dictionary\n"
     ]
    }
   ],
   "source": [
    "word = 'fall'\n",
    "\n",
    "if word in lmdict['Negative']:\n",
    "    print(f'Yes, {word} is a Negative word in the Loughran & McDonald dictionary')\n",
    "elif word in lmdict['Positive']:\n",
    "    print(f'Yes, {word} is Positive word in the Loughran & McDonald dictionary')\n",
    "else:\n",
    "    print(f'No, {word} is not in the Loughran & McDonald dictionary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f1465",
   "metadata": {},
   "source": [
    "Negation is another challenge that emerges using this approach. A techy fix is to check if the word is preceeded by a negating word in our list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48d384c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "negate = [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\", \"ain't\", \"aren't\", \"can't\",\n",
    "          \"couldn't\", \"daren't\", \"didn't\", \"doesn't\", \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\",\n",
    "          \"neither\", \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\", \"neednt\", \"needn't\",\n",
    "          \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\", \"oughtnt\", \"shant\", \"shouldnt\", \"wasnt\",\n",
    "          \"werent\", \"oughtn't\", \"shan't\", \"shouldn't\", \"wasn't\", \"weren't\", \"without\", \"wont\", \"wouldnt\", \"won't\",\n",
    "          \"wouldn't\", \"rarely\", \"seldom\", \"despite\", \"no\", \"nobody\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a0377ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def negated(word):\n",
    "    \"\"\"\n",
    "    Determine if preceding word is a negation word\n",
    "    \"\"\"\n",
    "    if word.lower() in negate:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcd91aa",
   "metadata": {},
   "source": [
    "This function counts the number of negative and positive words in a document and performs a negation check to switch the polarity of words that are preceeded by a word in the *negate* list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ec407be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results with negation check:\n",
      "\n",
      "The # of positive words: 0\n",
      "The # of negative words: 6\n",
      "The list of found positive words: []\n",
      "The list of found negative words: ['worst', 'worse', 'recession', 'fears', 'recessions', 'decline']\n",
      "The overall classification is: negative\n"
     ]
    }
   ],
   "source": [
    "def tone_count_with_negation_check(dict, article):\n",
    "    \"\"\"\n",
    "    Count positive and negative words with negation check. Account for simple negation only for positive words.\n",
    "    Simple negation is taken to be observations of one of negate words occurring within three words\n",
    "    preceding a positive words.\n",
    "    \"\"\"\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    " \n",
    "    pos_words = []\n",
    "    neg_words = []\n",
    " \n",
    "    input_words = re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', article.lower())\n",
    " \n",
    "    word_count = len(input_words)\n",
    " \n",
    "    for i in range(0, word_count):\n",
    "        if input_words[i] in dict['Negative']:\n",
    "            neg_count += 1\n",
    "            neg_words.append(input_words[i])\n",
    "        if input_words[i] in dict['Positive']:\n",
    "            if i >= 3:\n",
    "                if negated(input_words[i - 1]) or negated(input_words[i - 2]) or negated(input_words[i - 3]):\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i])\n",
    "            elif i == 2:\n",
    "                if negated(input_words[i - 1]) or negated(input_words[i - 2]):\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i])\n",
    "            elif i == 1:\n",
    "                if negated(input_words[i - 1]):\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i])\n",
    "            elif i == 0:\n",
    "                pos_count += 1\n",
    "                pos_words.append(input_words[i])\n",
    "                \n",
    "    sentiment_score = len(pos_words) - len(neg_words)\n",
    "    \n",
    "    if sentiment_score < 0:\n",
    "        classification = 'negative'\n",
    "    elif sentiment_score > 0:\n",
    "        classification = 'positive'\n",
    "    else:\n",
    "        classification = 'neutral'\n",
    " \n",
    "    results = [word_count, pos_count, neg_count, pos_words, neg_words, classification]\n",
    " \n",
    "    return results\n",
    " \n",
    "    \n",
    "# A sample output\n",
    "article = '''The stock market has had its worst start to a year in recent history and things could get \n",
    "             worse as recession fears loom. Since World War II there have been 13 recessions—defined as \n",
    "             two consecutive quarters of GDP decline–and there have been 3 in the 21st century (2001, 2008 and 2020), \n",
    "             according to the National Bureau of Economic Research. Some experts say another one could be on the way.'''\n",
    " \n",
    "sent_results = tone_count_with_negation_check(lmdict, article)\n",
    "\n",
    "print('The results with negation check:', end='\\n\\n')\n",
    "print('The # of positive words:', sent_results[1])\n",
    "print('The # of negative words:', sent_results[2])\n",
    "print('The list of found positive words:', sent_results[3])\n",
    "print('The list of found negative words:', sent_results[4])\n",
    "print('The overall classification is:', sent_results[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dee1da-6f30-491a-b31a-eb0e877862f7",
   "metadata": {},
   "source": [
    "Sometimes this technique can be successful, but there are many examples where it falls short. For example, when we use a clearly negative text that doesn't contain any words in the lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d2cdaf7-6459-4ad0-92a9-00f377034870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results with negation check:\n",
      "\n",
      "The # of positive words: 0\n",
      "The # of negative words: 0\n",
      "The list of found positive words: []\n",
      "The list of found negative words: []\n",
      "The overall classification is: neutral\n"
     ]
    }
   ],
   "source": [
    "article = '''Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that \n",
    "             were hit by larger expenditures on R&D and marketing'''\n",
    " \n",
    "sent_results = tone_count_with_negation_check(lmdict, article)\n",
    "\n",
    "print('The results with negation check:', end='\\n\\n')\n",
    "print('The # of positive words:', sent_results[1])\n",
    "print('The # of negative words:', sent_results[2])\n",
    "print('The list of found positive words:', sent_results[3])\n",
    "print('The list of found negative words:', sent_results[4])\n",
    "print('The overall classification is:', sent_results[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1627da33-0944-4089-88aa-c499aecb0b7f",
   "metadata": {},
   "source": [
    "#### Performance on the Financial PhraseBank dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "15ff9a6c-393a-4d33-961f-8fcfeb9eaa49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = list(df['class'])\n",
    "predictions= list()\n",
    "\n",
    "for phrase, c_i in zip(df['text'], df['class']):\n",
    "    sent_results = tone_count_with_negation_check(lmdict, phrase)\n",
    "    classification = sent_results[5]\n",
    "    predictions.append(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "711dba89-2fea-4410-82ed-93db1b99740d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We passed 873 examples through the LM classifier \n",
      "\n",
      "There were 573 NEUTRAL predictions\n",
      "There were 101 POSITIVE predictions\n",
      "There were 199 NEGATIVE predictions\n"
     ]
    }
   ],
   "source": [
    "neutral_count = predictions.count('neutral')\n",
    "positive_count = predictions.count('positive')\n",
    "negative_count = predictions.count('negative')\n",
    "\n",
    "print(f'We passed {len(predictions)} examples through the LM classifier \\n')\n",
    "print(f'There were {neutral_count} NEUTRAL predictions')\n",
    "print(f'There were {positive_count} POSITIVE predictions')\n",
    "print(f'There were {negative_count} NEGATIVE predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ecba44f5-cbe7-4ab8-b473-609628f1f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.613     0.403     0.486       303\n",
      "     neutral      0.000     0.000     0.000         0\n",
      "    positive      0.921     0.163     0.277       570\n",
      "\n",
      "    accuracy                          0.246       873\n",
      "   macro avg      0.511     0.189     0.254       873\n",
      "weighted avg      0.814     0.246     0.350       873\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\anaconda3\\envs\\OMI_NLP_workshop\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\felix\\anaconda3\\envs\\OMI_NLP_workshop\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\felix\\anaconda3\\envs\\OMI_NLP_workshop\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels, predictions, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3033173-1b0d-42b9-9a94-5e46ded9b4ef",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9589ae9f-842e-4b6e-89b7-2bd147d759f0",
   "metadata": {},
   "source": [
    "A NB classifier improves upon the lexicon technique explored above as it utilises the individual contribution of each word in the text. The fundamental training goal of a NB classifier is to calculate the individual probability of a word appearing in a particular class, $P(w|c_i)$, and summing the log probability of each word to discern $P(d|c_i)$. \n",
    "\n",
    "There are some assumptions and training details in our model:\n",
    "- Add-one smoothing - assume that all words appear once in each class so that one can calculate the probability for a word appearing.\n",
    "    - Large oversimplification but it is necessary for both classes to have the same support.\n",
    "- Remove all stop words - probability distribution of stop words doesn't neccessarily discern sentiment classification.\n",
    "    - she, he, it, so, I, etc. \n",
    "    \n",
    "##### Advantages and disadvatanges:\n",
    "    \n",
    "<font color='green'>$\\checkmark$</font>   Granular feature set \\\n",
    "<font color='green'>$\\checkmark$</font>   Can also include bigrams, trigrams, emojis etc. for improved performance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77298ce7-dc82-42a1-9103-240a38a4e9cb",
   "metadata": {},
   "source": [
    "#### Training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f1a95616-a1c6-407b-85c2-c82cec38e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes with additive smoothing\n",
    "# Define function to get P(w|c_i), class-conditional propbabilities for w\n",
    "\n",
    "def naive_bayes_additive_smoothing(vocab, categories, smoothing_alpha):\n",
    "    \n",
    "    # Calculate unsmoothed probabilities\n",
    "    probabilities = dict()\n",
    "    \n",
    "    for c_i in categories:\n",
    "        \n",
    "        probabilities[c_i] = dict()\n",
    "        \n",
    "        # First, consider all words that are in the vocab for either class\n",
    "        for word in set(vocab[\"negative\"]).union(set(vocab[\"positive\"])):\n",
    "            # If they do exist in the current class c_i, store their count --> 1st order model\n",
    "            if vocab[c_i][word]>0:\n",
    "                probabilities[c_i][word] = vocab[c_i][word]\n",
    "            # If not, set their count to be the smoothing parameter (rather than excluding them, as we did for no smoothing) --> backoff to 0th order model\n",
    "            else:\n",
    "                probabilities[c_i][word] = smoothing_alpha\n",
    "        \n",
    "        # Second, we take the sum of counts of words in this new dict\n",
    "        total = sum(probabilities[c_i].values())\n",
    "        \n",
    "        # Last, we turn the counts for each word into probabilities by dividing them by that sum\n",
    "        probabilities[c_i] = {word: probabilities[c_i][word] / total for word in probabilities[c_i]}\n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84ef887-ee24-4a55-b2c2-379affc1f327",
   "metadata": {},
   "source": [
    "Estimate the probability of the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f9d0695f-2de6-465a-8bf6-ca7e95ce8dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate P(c_i), the probability of class c_i, based on the class distribution in the test set\n",
    "prob_class = dict()\n",
    "for c_i in categories:\n",
    "    prob_class[c_i] = train_text[c_i].shape[0]/(train_text[\"negative\"].shape[0]+train_text[\"positive\"].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4848084-5a79-4d05-afff-827da94b4f18",
   "metadata": {},
   "source": [
    "Retrieve the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "993c87f6-c92a-43b2-9b4d-ecd94c775cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_predictions(categories, test_tweets, probabilities, prob_class):\n",
    "\n",
    "    # Initialize lists for storing ground truth labels and predictions\n",
    "    labels = list()\n",
    "    predictions = list()\n",
    "\n",
    "    # Loop over categories\n",
    "    for c_i in categories:\n",
    "\n",
    "        # Loop over test tweets\n",
    "        for tweet in test_tweets[c_i]:\n",
    "\n",
    "            # Store ground truth\n",
    "            labels.append(c_i)\n",
    "\n",
    "            # For each post, calculate scores for each of the two categories\n",
    "            scores = {'negative': 0, 'positive': 0}\n",
    "            for word in tweet:\n",
    "                if word in probabilities[c_i]:\n",
    "                    scores[\"negative\"] += np.log(probabilities[\"negative\"][word])\n",
    "                    scores[\"positive\"] += np.log(probabilities[\"positive\"][word])\n",
    "\n",
    "            # Class imbalance\n",
    "            scores[\"negative\"] = scores[\"negative\"]+np.log(prob_class[\"negative\"])\n",
    "            scores[\"positive\"] = scores[\"positive\"]+np.log(prob_class[\"positive\"])\n",
    "\n",
    "            # Use higher score for prediction\n",
    "            predictions.append(max(scores.items(), key=operator.itemgetter(1))[0])\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20dc299-805b-4d9a-9bb5-50e542fd54b3",
   "metadata": {},
   "source": [
    "### Exercise: tuning the smoothing-alpha parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cd785e-e0fb-464c-aa31-c6ff1269553f",
   "metadata": {},
   "source": [
    "For this exercise you should try and find the best smoothing-alpha value that will result in the highest macro-F1 score.\n",
    "\n",
    "\n",
    "Step 1) Create a data object of smoothing-alpha values that we can loop over. What values and steps seem reasonable/are possible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6587d2ae-f94f-4cc2-8ffb-9c1f0a5d4acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_alphas = [0.0000000001]+[x * 0.05 for x in range(1, 21)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade31c73-f78f-4d51-b0bc-718ac4ed26d4",
   "metadata": {},
   "source": [
    "Step 2) Loop over the smoothing-alpha values to calculate the macro-f1 score at each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a12738db-5639-45d5-b6b7-124c6b7a1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_f1_dict = dict()\n",
    "\n",
    "for smoothing_alpha in smoothing_alphas:\n",
    "    \n",
    "    probs = naive_bayes_additive_smoothing(vocab, categories, smoothing_alpha)\n",
    "    \n",
    "    labels, predictions = get_nb_predictions(categories, dev_text, probs, prob_class)\n",
    "    \n",
    "    macro_f1_dict[smoothing_alpha] = f1_score(labels, predictions, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d47bb-f622-46c7-9182-f10bd877beb5",
   "metadata": {},
   "source": [
    "Step 3) Find the best and worst smoothing-alpha values and identify the difference between the respective macro-f1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6e18677b-72bd-4299-80d4-f25c31374849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing parameter 0.75 produces the HIGHEST macro F1 on the dev set: 0.7833333333333334\n",
      "Smoothing parameter 1e-10 produces the LOWEST macro F1 on the dev set: 0.7322437047372268\n",
      "The difference between the highest and lowest macro F1 is 0.05108962859610666.\n"
     ]
    }
   ],
   "source": [
    "best_param, worst_param = max(macro_f1_dict, key=macro_f1_dict.get), min(macro_f1_dict, key=macro_f1_dict.get)\n",
    "best_f1, worst_f1 = max(macro_f1_dict.values()), min(macro_f1_dict.values())\n",
    "\n",
    "print(f\"Smoothing parameter {best_param} produces the HIGHEST macro F1 on the dev set: {best_f1}\")\n",
    "print(f\"Smoothing parameter {worst_param} produces the LOWEST macro F1 on the dev set: {worst_f1}\")\n",
    "print(f\"The difference between the highest and lowest macro F1 is {best_f1-worst_f1}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d06788d-87d9-44cf-8867-b8f84e6f0378",
   "metadata": {},
   "source": [
    "Step 4) Plot smoothing-alphas vs. macro-f1 socre so we can visualise and interpret the best smoothing-alphas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3108d52f-ffb4-4ff2-995e-345cdd09ecbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(12, 6))\n",
    "plt_x, plt_y = list(), list()\n",
    "\n",
    "for key in macro_f1_dict:\n",
    "    plt_x.append(key)\n",
    "    plt_y.append(macro_f1_dict[key])\n",
    "\n",
    "ax.plot(plt_x, plt_y)\n",
    "\n",
    "ax.set_title('Classification performance as a function of smoothing parameter alpha')\n",
    "ax.set_xlabel('Smoothing parameter alpha')\n",
    "ax.set_ylabel('Macro F1 score')\n",
    "ax.set_xticks(smoothing_alphas)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64de123b-6e42-4f55-bfb5-236aaacfb7a1",
   "metadata": {},
   "source": [
    "Step 5) Input the best performing smoothing-alpha to make our predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "65c59178-f511-4404-9c29-1e9e1dbda016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.625     0.652     0.638        46\n",
      "    positive      0.810     0.791     0.800        86\n",
      "\n",
      "    accuracy                          0.742       132\n",
      "   macro avg      0.717     0.721     0.719       132\n",
      "weighted avg      0.745     0.742     0.744       132\n",
      "\n",
      "0.7191489361702128\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "probs = naive_bayes_additive_smoothing(vocab, categories, smoothing_alpha=best_param)\n",
    "\n",
    "# Get predictions on dev set\n",
    "labels, predictions = get_nb_predictions(categories, test_text, probs, prob_class)\n",
    "\n",
    "# Calculate and store macro F1 on test set\n",
    "print(classification_report(labels, predictions, digits=3))\n",
    "print(f1_score(labels, predictions, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f930574",
   "metadata": {},
   "source": [
    "## 3. BERT classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03516dc1-abee-498f-a856-804b47983c23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import all dependencies \n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import  TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from transformers import AutoConfig\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91227355-8b5c-4bb6-bcc8-2485bdb4949c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the dataset from huggingfaces' dataset repository\n",
    "#fin_dataset = load_dataset('financial_phrasebank', 'sentences_allagree')\n",
    "#df = pd.DataFrame(fin_dataset['train']) # send  it to a pandas dataframe\n",
    "\n",
    "# HUGE INSTALL PROBLEM ON WINDOWS NEED TO FIX - RUNS FINE ON MAC - \n",
    "\n",
    "# MEANTIME INSTALL FROM data/FinancialPhraseBanl-v1.0\n",
    "\n",
    "df = pd.read_csv('data\\\\FinancialPhraseBank-v1.0\\\\Sentences_50Agree.txt',\n",
    "            encoding = 'ISO-8859-1',on_bad_lines='skip',sep = '.@')\n",
    "df.columns = ['sentence','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9054771-2667-4ab2-9d5f-162dee478a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].replace(to_replace=({'neutral':0,'positive':1,'negative':2}))\n",
    "df['label']= df['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "228a2858",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22345d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "675e1335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is our input sentence : \n",
      " Hi my name is BERT and I am overjoyed  to meet you ! \n",
      "\n",
      "These are the outputs of the tokenizer:\n",
      "\n",
      "{'input_ids': tensor([[  101,  8790,  1139,  1271,  1110,   139,  9637,  1942,  1105,   146,\n",
      "          1821,  1166, 18734,  1174,  1106,  2283,  1128,   106,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "\n",
      "These inputs correspond to the original sentence with separation and padding thrown in :\n",
      "\n",
      "['[CLS] Hi my name is BERT and I am overjoyed to meet you! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n"
     ]
    }
   ],
   "source": [
    "#how does the tokenizer work ? \n",
    "print('\\nThis is our input sentence : \\n Hi my name is BERT and I am overjoyed  to meet you ! \\n')\n",
    "\n",
    "out = tokenizer(['Hi my name is BERT and I am overjoyed  to meet you ! '],\n",
    "          max_length=64,padding=\"max_length\", truncation=True,return_tensors='pt')\n",
    "print('These are the outputs of the tokenizer:\\n')\n",
    "print(out)\n",
    "\n",
    "print('\\nThese inputs correspond to the original sentence with separation and padding thrown in :\\n')\n",
    "print([tokenizer.decode(i) for i in out['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ed749a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is our model : \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " First layer shape (vocabulary size) : \n",
      "  torch.Size([28996, 768]) \n",
      " Last layer shape (prediction task output shape) : \n",
      "  torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Now that we covered  the tokenizer lets introduce the other building block : the model \n",
    "\n",
    "print('this is our model : \\n')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased')\n",
    "layers = [i for i in model.parameters()]\n",
    "print('\\n First layer shape (vocabulary size) : \\n ',layers[0].shape,\n",
    "'\\n Last layer shape (prediction task output shape) : \\n ',layers[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9c745e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is our forward propagation syntax. \n",
      " We feed in a tokenized text and receive the \n",
      " predicted  logits over the 2 classes : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[0.5558, 0.0876]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic forward propagation of our BERT model \n",
    "print('This is our forward propagation syntax. \\n We feed in a tokenized text and receive the \\n predicted  logits over the 2 classes : \\n')\n",
    "model.forward(**out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebec1e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with BERT hands-on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baf53dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  define tokenizer & model \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# turn the configuration for a 3 sentiment classification task\n",
    "config = AutoConfig.from_pretrained('bert-base-cased')\n",
    "config.num_labels = 3\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_config(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "966ed0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.25, random_state=96)\n",
    "test, val = train_test_split(test, test_size=0.4, random_state=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b38988d-5ed7-4eaa-9c28-6c6ddef1ba4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.593174\n",
      "1    0.282136\n",
      "2    0.124690\n",
      "Name: label, dtype: float64 \n",
      " 0    0.583219\n",
      "1    0.294360\n",
      "2    0.122421\n",
      "Name: label, dtype: float64 \n",
      " 0    0.616495\n",
      "1    0.255670\n",
      "2    0.127835\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train.label.value_counts(normalize=True),'\\n',\n",
    "test.label.value_counts(normalize=True),'\\n',\n",
    "val.label.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "423f2be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a Dataset object to put our data in\n",
    "\n",
    "\n",
    "class BERTTutorialDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Special dataset class built on top of the torch Dataset class\n",
    "    useful to have memory efficient dataloading tokenization batching and trainning.\n",
    "    \n",
    "    Huggingface can use these types of dataset as inputs and run all trainning/prediction on them. \n",
    "    \"\"\"\n",
    "    def __init__(self, input_data, sentiment_targets, tokenizer, max_len):\n",
    "        \"\"\"\n",
    "        Basic generator function for the class.\n",
    "        -----------------\n",
    "        input_data : array\n",
    "            Numpy array of string  input text to use for downstream task \n",
    "        sentiment_targets : \n",
    "            Numpy array of integers indexed in  the pytorch style of [0,C-1] with C being the total number of classes\n",
    "            In our example this means the target sentiments should range from 0 to 2. \n",
    "        tokenizer  : Huggingface tokenizer \n",
    "            The huggingface tokenizer to use\n",
    "        max_len : \n",
    "            The truncation length of the tokenizer \n",
    "        -------------------\n",
    "        \n",
    "        Returns : \n",
    "        \n",
    "            Tokenized text with inputs, attentions and labels, ready for the Training script. \n",
    "        \"\"\"\n",
    "        self.input_data = input_data\n",
    "        self.sentiment_targets = sentiment_targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Function required by torch huggingface to batch efficiently\n",
    "        \"\"\"\n",
    "        return len(self.input_data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.input_data[item])\n",
    "        target = self.sentiment_targets[item]\n",
    "        # only difference with the previuous tokenization step is the encode-plus for special tokens\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          text,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          return_token_type_ids=False,\n",
    "          padding='max_length',\n",
    "          return_attention_mask=True,\n",
    "          return_tensors='pt',\n",
    "          truncation = True\n",
    "        )\n",
    "        return {\n",
    "          'text': text,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'labels': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a52df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our train-val-test datasets\n",
    "MAX_LEN = 128\n",
    "train_ds = BERTTutorialDataset(\n",
    "    input_data=train['sentence'].to_numpy(),\n",
    "        sentiment_targets=train['label'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "val_ds = BERTTutorialDataset(\n",
    "    input_data=val['sentence'].to_numpy(),\n",
    "        sentiment_targets=val['label'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "\n",
    "test_ds = BERTTutorialDataset(\n",
    "    input_data=test['sentence'].to_numpy(),\n",
    "        sentiment_targets=test['label'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfc2a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some accuracy measure ( helpful for the early stopping )\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import numpy as np\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    Function to calculate accuracies and losses for the validation from the predicted outputs\n",
    "    This is neccessary for the early stopping. \n",
    "    \"\"\"\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred, average='macro')\n",
    "    precision = precision_score(y_true=labels, y_pred=pred, average='macro')\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred, average='macro')    \n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71b300ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# Define trainning arguments \n",
    "training_args = TrainingArguments('BERT_TUTORIAL_MODEL', overwrite_output_dir=True, evaluation_strategy=\"steps\", \n",
    "                                  num_train_epochs=50, weight_decay=1e-8,learning_rate=1e-5,\n",
    "                                  eval_steps=50,metric_for_best_model='accuracy',\n",
    "                                 per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "                                 load_best_model_at_end = True, save_total_limit=10, save_steps=50,no_cuda=False,\n",
    "                             fp16=True,gradient_accumulation_steps=4)\n",
    "trainer = Trainer(\n",
    "    model =model, args=training_args, train_dataset=train_ds, eval_dataset=val_ds,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=5)], compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf13f2e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3633\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 2850\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='2850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/2850 21:12 < 36:26, 0.82 it/s, Epoch 18/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.928814</td>\n",
       "      <td>0.616495</td>\n",
       "      <td>0.205498</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.254252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.856737</td>\n",
       "      <td>0.647423</td>\n",
       "      <td>0.368049</td>\n",
       "      <td>0.373656</td>\n",
       "      <td>0.329069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.788579</td>\n",
       "      <td>0.661856</td>\n",
       "      <td>0.536953</td>\n",
       "      <td>0.461511</td>\n",
       "      <td>0.446089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.838215</td>\n",
       "      <td>0.628866</td>\n",
       "      <td>0.388787</td>\n",
       "      <td>0.453303</td>\n",
       "      <td>0.414593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>0.684536</td>\n",
       "      <td>0.640621</td>\n",
       "      <td>0.465971</td>\n",
       "      <td>0.477195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.957413</td>\n",
       "      <td>0.659794</td>\n",
       "      <td>0.743213</td>\n",
       "      <td>0.435484</td>\n",
       "      <td>0.409800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.770547</td>\n",
       "      <td>0.682474</td>\n",
       "      <td>0.742312</td>\n",
       "      <td>0.462950</td>\n",
       "      <td>0.441057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.751887</td>\n",
       "      <td>0.686598</td>\n",
       "      <td>0.581833</td>\n",
       "      <td>0.516911</td>\n",
       "      <td>0.529841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.952682</td>\n",
       "      <td>0.668041</td>\n",
       "      <td>0.584543</td>\n",
       "      <td>0.457906</td>\n",
       "      <td>0.446611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.744500</td>\n",
       "      <td>0.781731</td>\n",
       "      <td>0.680412</td>\n",
       "      <td>0.650079</td>\n",
       "      <td>0.504450</td>\n",
       "      <td>0.519265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.744500</td>\n",
       "      <td>0.831976</td>\n",
       "      <td>0.668041</td>\n",
       "      <td>0.651187</td>\n",
       "      <td>0.531719</td>\n",
       "      <td>0.523404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.744500</td>\n",
       "      <td>0.840635</td>\n",
       "      <td>0.676289</td>\n",
       "      <td>0.663254</td>\n",
       "      <td>0.527196</td>\n",
       "      <td>0.519654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.744500</td>\n",
       "      <td>0.830610</td>\n",
       "      <td>0.692784</td>\n",
       "      <td>0.676006</td>\n",
       "      <td>0.532249</td>\n",
       "      <td>0.550324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.744500</td>\n",
       "      <td>0.901900</td>\n",
       "      <td>0.709278</td>\n",
       "      <td>0.668698</td>\n",
       "      <td>0.532582</td>\n",
       "      <td>0.562010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.744500</td>\n",
       "      <td>0.916970</td>\n",
       "      <td>0.686598</td>\n",
       "      <td>0.589772</td>\n",
       "      <td>0.605414</td>\n",
       "      <td>0.596297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.744500</td>\n",
       "      <td>0.883267</td>\n",
       "      <td>0.719588</td>\n",
       "      <td>0.677453</td>\n",
       "      <td>0.619251</td>\n",
       "      <td>0.629903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.744500</td>\n",
       "      <td>0.864733</td>\n",
       "      <td>0.701031</td>\n",
       "      <td>0.655197</td>\n",
       "      <td>0.566080</td>\n",
       "      <td>0.590876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.744500</td>\n",
       "      <td>0.889107</td>\n",
       "      <td>0.709278</td>\n",
       "      <td>0.647875</td>\n",
       "      <td>0.593879</td>\n",
       "      <td>0.612588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.744500</td>\n",
       "      <td>0.979369</td>\n",
       "      <td>0.711340</td>\n",
       "      <td>0.704813</td>\n",
       "      <td>0.560902</td>\n",
       "      <td>0.587601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.409000</td>\n",
       "      <td>0.987336</td>\n",
       "      <td>0.709278</td>\n",
       "      <td>0.705875</td>\n",
       "      <td>0.560902</td>\n",
       "      <td>0.590083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.409000</td>\n",
       "      <td>0.968426</td>\n",
       "      <td>0.713402</td>\n",
       "      <td>0.658292</td>\n",
       "      <td>0.601746</td>\n",
       "      <td>0.617260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-50\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-50\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-50\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-100\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-100\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-100\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-150\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-150\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-150\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-200\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-200\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-200\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-350] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-250\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-250\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-250\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-300\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-300\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-300\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-450] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-350\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-350\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-350\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-400\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-400\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-400\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-550] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-450\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-450\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-450\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-500\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-500\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-500\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-650] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-550\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-550\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-550\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-600\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-600\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-600\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-650\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-650\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-650\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-150] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-700\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-700\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-700\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-750\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-750\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-750\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-800\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-800\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-800\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-850\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-850\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-850\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-350] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-900\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-900\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-900\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-950\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-950\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-950\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-450] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-1000\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-1000\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-1000\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-1050\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-1050\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-1050\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-550] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from BERT_TUTORIAL_MODEL\\checkpoint-800 (score: 0.7195876288659794).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1050, training_loss=0.5613758604867117, metrics={'train_runtime': 1276.1648, 'train_samples_per_second': 142.341, 'train_steps_per_second': 2.233, 'total_flos': 4402545262133760.0, 'train_loss': 0.5613758604867117, 'epoch': 18.42})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "899ef4cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 727\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='92' max='46' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [46/46 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.963792622089386,\n",
       " 'eval_accuracy': 0.6671251719394773,\n",
       " 'eval_precision': 0.6387364719011513,\n",
       " 'eval_recall': 0.5910578460489896,\n",
       " 'eval_f1': 0.5973725368696158,\n",
       " 'eval_runtime': 6.2896,\n",
       " 'eval_samples_per_second': 115.588,\n",
       " 'eval_steps_per_second': 7.314,\n",
       " 'epoch': 18.42}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79371fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 727\n",
      "  Batch size = 16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "predictions = trainer.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68ae1519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD8CAYAAABJsn7AAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAP5klEQVR4nO3df6jd9X3H8ecr1tmtdkxxSpqk1K1xnXYsbi4MhOHmVl0ZxA6EOKhhy3Y70E2hfxgLW9uVDAerZbC17HZKU9YpgbYYSulmpaW4tmoU1xpTZ1DRazLDaofVP1zvPe/9cb/Bs/Tec869nptP7tfnQz6c7/l8f3w+XsI777y/n+/3pqqQJJ16G1pPQJLeqAzAktSIAViSGjEAS1IjBmBJasQALEmNGIAlaQlJ3pzkwST/keRQko92/ecmuTfJk93nOUPn3JrkSJInklw1dgzXAUvSj0sS4C1V9XKSM4H7gZuA3wderKrbkuwBzqmqW5JcDNwFbAfeBnwVuKiqFpYbwwxYkpZQi17uvp7ZtQJ2APu6/n3ANd32DuDuqnq1qp4GjrAYjJf1pmlP+mQ/+u+nTLHX2N/9yl+2nsIbwl99/5utp9B7L73yVF7vNVYSc8487+dGjpfkDOBh4J3AP1TVA0kuqKpjAFV1LMn53eGbgG8PnT7X9S3LDFhSvwwWJm5JZpIcHGozw5eqqoWq2gZsBrYnefeIkZcK5iP/MljzDFiSTqkaTH5o1SwwO8Fx/5Pk68DVwAtJNnbZ70bgeHfYHLBl6LTNwNFR1zUDltQvg8HkbYQkP5vkZ7rtnwR+G/gecADY1R22C7in2z4A7ExyVpILga3Ag6PGMAOW1Cu1ggx4jI3Avq4OvAHYX1VfSvItYH+S3cCzwLWL49ahJPuBx4F54IZRKyDAACypbxbmp3KZqvoOcOkS/d8HrlzmnL3A3knHMABL6pfByKTztGIAltQv0ytBrDkDsKR+GXNz7XRiAJbUK1O8CbfmDMCS+sUMWJIaWfhR6xlMzAAsqV8sQUhSI5YgJKkRM2BJasQMWJLaqIE34SSpDTNgSWrEGrAkNeLLeCSpETNgSWrEGrAkNTKlF7KfCgZgSf1iBixJbYz5NWynFQOwpH4xA5akRlwFIUmNmAFLUiOugpCkRixBSFIjliAkqREDsCQ1YglCkhpZRzfhNrSegCRN1WAweRshyZYkX0tyOMmhJDd1/R9J8nySR7v23qFzbk1yJMkTSa4aN9WxGXCSdwE7gE1AAUeBA1V1eNy5knTKTa8EMQ98sKoeSfJW4OEk93b7PlFVfzt8cJKLgZ3AJcDbgK8muahGPBs9MgNOcgtwNxDgQeChbvuuJHtW+T8lSWtnShlwVR2rqke67R8Ch1lMRJezA7i7ql6tqqeBI8D2UWOMK0HsBn6tqm6rqn/u2m3dRXcvd1KSmSQHkxz8p8/eNWYISZqiKQXgYUneAVwKPNB13ZjkO0nuTHJO17cJeG7otDlGB+yxAXjAYip9so3dviVV1WxVXVZVl/3x9deNGUKSpqhq4jacLHZt5uTLJTkb+Dxwc1W9BHwK+HlgG3AM+PiJQ5eazaipjqsB3wzcl+RJXovsbwfeCdw45lxJOvXmJ18FUVWzwOxy+5OcyWLw/VxVfaE754Wh/Z8GvtR9nQO2DJ2+mcV7ZssaGYCr6itJLmKx5LCJxQg/Bzw0qrAsSc1M6SZckgB3AIer6vah/o1Vdaz7+j7gsW77APAvSW5nsXKwlcV7Z8sauwqiqgbAt1c+fUlqYHpPwl0OvB/4bpJHu74PAdcl2cZieeEZ4AMAVXUoyX7gcRZXUNwwLlH1QQxJ/VIjy64ruEzdz9J13S+POGcvsHfSMQzAkvrFd0FIUiMGYElqoxbWz/oAA7CkfjEDlqRGfB2lJDUymM4qiFPBACypXyxBSFIj3oSTpEbMgCWpEWvAktSIqyAkqREzYElqo6wBS1IjroKQpEYsQUhSI5YgJKkRM2BJasRlaJLUiBmwJLVR866CkKQ2zIAlqRFrwJLUiBmwJLVRBmBJasSbcJLUiBmwJDViAJakNqrWTwDe0HoCkjRVg5q8jZBkS5KvJTmc5FCSm7r+c5Pcm+TJ7vOcoXNuTXIkyRNJrho3VQOwpH6ZUgAG5oEPVtUvAr8O3JDkYmAPcF9VbQXu677T7dsJXAJcDXwyyRmjBljzEsTV2/50rYd4w/sbfqr1FN4Q9vzo1dZT0ARqfjoPYlTVMeBYt/3DJIeBTcAO4IrusH3A14Fbuv67q+pV4OkkR4DtwLeWG8MMWFK/DFbQJpTkHcClwAPABV1wPhGkz+8O2wQ8N3TaXNe3LAOwpF6pQU3ckswkOTjUZk6+XpKzgc8DN1fVSyOGzlLTGTVXV0FI6pcVLEOrqllgdrn9Sc5kMfh+rqq+0HW/kGRjVR1LshE43vXPAVuGTt8MHB01vhmwpH6ZUgkiSYA7gMNVdfvQrgPArm57F3DPUP/OJGcluRDYCjw4agwzYEm9MsV3QVwOvB/4bpJHu74PAbcB+5PsBp4FrgWoqkNJ9gOPs7iC4oaqGvlctAFYUq/U/HQCcFXdz9J1XYArlzlnL7B30jEMwJL6Zf28DtgALKlf1tH72A3AknrGACxJbZgBS1IjNd96BpMzAEvqFTNgSWrEACxJrdRyS3dPPwZgSb1iBixJjdTADFiSmhgsGIAlqQlLEJLUiCUISWpkHf1WegOwpH4xA5akRrwJJ0mNmAFLUiPlk3CS1IbL0CSpkYEZsCS1YQlCkhpxFYQkNeIqCElqxBqwJDViDViSGvFdEJLUyHoqQWxoPQFJmqbBIBO3cZLcmeR4kseG+j6S5Pkkj3btvUP7bk1yJMkTSa4ad/1VB+Akf7jacyVprQwqE7cJfAa4eon+T1TVtq59GSDJxcBO4JLunE8mOWPUxV9PBvzR5XYkmUlyMMnB51+Zex1DSNLKVGXiNv5a9Q3gxQmH3gHcXVWvVtXTwBFg+6gTRtaAk3xnuV3ABcudV1WzwCzAlZvfs45K4pLWu1NUA74xyfXAQeCDVfUDYBPw7aFj5rq+ZY27CXcBcBXwg5P6A3xzRdOVpFNgJRlfkhlgZqhrtksgR/kU8LFuqI8BHwf+iMW4uKLpjAvAXwLOrqpHT96R5OtjzpWkU25hMHlldfhf6ys454UT20k+zWKchMWMd8vQoZuBo6OuNXKmVbW7qu5fZt8fTDRbSTqFBitoq5Fk49DX9wEnVkgcAHYmOSvJhcBW4MFR13IdsKReqSUrAauT5C7gCuC8JHPAh4ErkmxjsbzwDPABgKo6lGQ/8DgwD9xQVQujrm8AltQrgyne9q+q65bovmPE8XuBvZNe3wAsqVcGU8yA15oBWFKvTLMEsdYMwJJ6ZcEALEltrKPfyWkAltQvBmBJasQasCQ1so5+JZwBWFK/uAxNkhoZ+ejZacYALKlXBjEDlqQm1tMLyA3AknrFZWiS1IirICSpER9FlqRGzIAlqRFrwJLUiKsgJKkRSxCS1IglCElqZMEMWJLaMAOWpEYMwJLUiKsgJKkRV0FIUiOWICSpEV/ILkmNWIKQpEbWUwliQ+sJSNI01QraOEnuTHI8yWNDfecmuTfJk93nOUP7bk1yJMkTSa4ad/01z4C/98rzaz3EG951b3pz6ym8Ibz9p89vPQVNYDDdhWifAf4e+OxQ3x7gvqq6Lcme7vstSS4GdgKXAG8DvprkoqpatixtBiypVxZW0Mapqm8AL57UvQPY123vA64Z6r+7ql6tqqeBI8D2Udc3AEvqlcEK2ipdUFXHALrPE/802gQ8N3TcXNe3LAOwpF4ZZPKWZCbJwaE28zqGXmr9xch6iKsgJPXKSmrAVTULzK5wiBeSbKyqY0k2Ase7/jlgy9Bxm4Gjoy5kBiypV6a5CmIZB4Bd3fYu4J6h/p1JzkpyIbAVeHDUhcyAJfXKNNcBJ7kLuAI4L8kc8GHgNmB/kt3As8C1AFV1KMl+4HFgHrhh1AoIMABL6pmFKS5Dq6rrltl15TLH7wX2Tnp9A7CkXllPT8IZgCX1ypQfxFhTBmBJvbJ+wq8BWFLPWIKQpEameRNurRmAJfWKNWBJamT9hF8DsKSeMQOWpEa8CSdJjZQZsCS14SoISWrEEoQkNTIoM2BJamL9hF8DsKSecRmaJDXiKghJamTeACxJbZgBS1IjLkOTpEbKZWiS1IarICSpER9FlqRGzIAlqRFrwJLUiKsgJKkR1wFLUiPrqQa8YdwBSd6V5MokZ5/Uf/XaTUuSVmehBhO31kYG4CR/DtwD/BnwWJIdQ7v/ei0nJkmrUSv4b5wkzyT5bpJHkxzs+s5Ncm+SJ7vPc1Y713EZ8J8Av1pV1wBXAH+R5KYTcxsx6ZkkB5McfOXVH6x2bpK0YoOqiduEfrOqtlXVZd33PcB9VbUVuK/7virjAvAZVfUyQFU9w2IQ/t0ktzMiAFfVbFVdVlWXveWsVf/lIEkrVitoq7QD2Ndt7wOuWe2FxgXg/0qy7cSXLhj/HnAe8EurHVSS1sqAmrhNoIB/S/Jwkpmu74KqOgbQfZ6/2rmOWwVxPTD//2ZTNQ9cn+QfVzuoJK2VlayC6ILqzFDXbFXNDn2/vKqOJjkfuDfJ96Y0TWBMAK6quRH7/n2aE5GkaVjJ6oYu2M6O2H+0+zye5IvAduCFJBur6liSjcDx1c517DI0SVpPprUKIslbkrz1xDbwHuAx4ACwqztsF4srxVbFBzEk9coU3wVxAfDFJLAYK/+lqr6S5CFgf5LdwLPAtasdwAAsqVem9SRcVT0F/PIS/d8HrpzGGAZgSb3i29AkqZGFdfQ+NAOwpF5ZwRNuzRmAJfWKr6OUpEbMgCWpETNgSWrEDFiSGjkdXrQ+KQOwpF6xBCFJjZQZsCS1sZ5+KacBWFKv+CiyJDViBixJjSwMrAFLUhOugpCkRqwBS1Ij1oAlqREzYElqxJtwktSIJQhJasQShCQ14usoJakR1wFLUiNmwJLUyMDXUUpSG96Ek6RGDMCS1Mj6Cb+Q9fS3xamSZKaqZlvPo8/8Ga89f8anvw2tJ3Cammk9gTcAf8Zrz5/xac4ALEmNGIAlqRED8NKsm609f8Zrz5/xac6bcJLUiBmwJDViAB6S5OokTyQ5kmRP6/n0UZI7kxxP8ljrufRVki1JvpbkcJJDSW5qPSctzRJEJ8kZwH8CvwPMAQ8B11XV400n1jNJfgN4GfhsVb279Xz6KMlGYGNVPZLkrcDDwDX+WT79mAG/ZjtwpKqeqqr/Be4GdjSeU+9U1TeAF1vPo8+q6lhVPdJt/xA4DGxqOystxQD8mk3Ac0Pf5/APrda5JO8ALgUeaDwVLcEA/Jos0Wd9RutWkrOBzwM3V9VLreejH2cAfs0csGXo+2bgaKO5SK9LkjNZDL6fq6ovtJ6PlmYAfs1DwNYkFyb5CWAncKDxnKQVSxLgDuBwVd3eej5angG4U1XzwI3Av7J402J/VR1qO6v+SXIX8C3gF5LMJdndek49dDnwfuC3kjzatfe2npR+nMvQJKkRM2BJasQALEmNGIAlqREDsCQ1YgCWpEYMwJLUiAFYkhoxAEtSI/8H8Z0Cn53LEF4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = np.argmax(predictions.predictions,1)\n",
    "sns.heatmap(confusion_matrix(test.label.values,output))#,labels = ['1','-1','0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5168830c-7a43-4de3-837c-8fae2ad2efc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[301, 108,  15],\n",
       "       [ 56, 153,   5],\n",
       "       [ 25,  33,  31]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test.label.values,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cbfe5c9-5e8d-44bc-a18b-9f63d530b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "del train_ds\n",
    "del val_ds\n",
    "del test_ds\n",
    "del predictions\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad063e57-d68f-4a52-9221-1fdd253950bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/ProsusAI/finbert/resolve/main/config.json from cache at C:\\Users\\drago/.cache\\huggingface\\transformers\\2120f4f96b5830e5a91fe94d242471b0133b0976c8d6e081594ab837ac5f17bc.ef97278c578016c8bb785f15296476b12eae86423097fed78719d1c8197a3430\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ProsusAI/finbert\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"positive\",\n",
      "    \"1\": \"negative\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 1,\n",
      "    \"neutral\": 2,\n",
      "    \"positive\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/ProsusAI/finbert/resolve/main/vocab.txt from cache at C:\\Users\\drago/.cache\\huggingface\\transformers\\a5b1a5451c9cf1702eec1072ac325d4af10e675a654628eab453b8cba2c6b111.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/ProsusAI/finbert/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/ProsusAI/finbert/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/ProsusAI/finbert/resolve/main/special_tokens_map.json from cache at C:\\Users\\drago/.cache\\huggingface\\transformers\\4c21e8896b03f68c2e028133cf579267c62aba9de03a704a0845704e58eefe9e.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/ProsusAI/finbert/resolve/main/tokenizer_config.json from cache at C:\\Users\\drago/.cache\\huggingface\\transformers\\e3709a60694f45adca209a405cc69ce2b5d47b1cae60696ed9a901426be8c43d.8b6dccc90d16201c6d7ab0f3c6cc38e74b5f2fe587f6efadc9fa71fc0a00c606\n",
      "loading configuration file https://huggingface.co/ProsusAI/finbert/resolve/main/config.json from cache at C:\\Users\\drago/.cache\\huggingface\\transformers\\2120f4f96b5830e5a91fe94d242471b0133b0976c8d6e081594ab837ac5f17bc.ef97278c578016c8bb785f15296476b12eae86423097fed78719d1c8197a3430\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ProsusAI/finbert\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"positive\",\n",
      "    \"1\": \"negative\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 1,\n",
      "    \"neutral\": 2,\n",
      "    \"positive\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/ProsusAI/finbert/resolve/main/config.json from cache at C:\\Users\\drago/.cache\\huggingface\\transformers\\2120f4f96b5830e5a91fe94d242471b0133b0976c8d6e081594ab837ac5f17bc.ef97278c578016c8bb785f15296476b12eae86423097fed78719d1c8197a3430\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ProsusAI/finbert\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"positive\",\n",
      "    \"1\": \"negative\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 1,\n",
      "    \"neutral\": 2,\n",
      "    \"positive\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/ProsusAI/finbert/resolve/main/config.json from cache at C:\\Users\\drago/.cache\\huggingface\\transformers\\2120f4f96b5830e5a91fe94d242471b0133b0976c8d6e081594ab837ac5f17bc.ef97278c578016c8bb785f15296476b12eae86423097fed78719d1c8197a3430\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ProsusAI/finbert\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"positive\",\n",
      "    \"1\": \"negative\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 1,\n",
      "    \"neutral\": 2,\n",
      "    \"positive\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try with finbert \n",
    "#  define tokenizer & model --> this is just a change from the previous code above\n",
    "finbert_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "# turn the configuration for a 3 sentiment classification task\n",
    "config = AutoConfig.from_pretrained(\"ProsusAI/finbert\")\n",
    "config.num_labels = 3\n",
    "\n",
    "finbert_model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "# redefine our datasets as wechanged the tokenizer \n",
    "\n",
    "MAX_LEN = 128\n",
    "train_ds = BERTTutorialDataset(\n",
    "    input_data=train['sentence'].to_numpy(),\n",
    "        sentiment_targets=train['label'].to_numpy(),\n",
    "        tokenizer=finbert_tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "val_ds = BERTTutorialDataset(\n",
    "    input_data=val['sentence'].to_numpy(),\n",
    "        sentiment_targets=val['label'].to_numpy(),\n",
    "        tokenizer=finbert_tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "\n",
    "test_ds = BERTTutorialDataset(\n",
    "    input_data=test['sentence'].to_numpy(),\n",
    "        sentiment_targets=test['label'].to_numpy(),\n",
    "        tokenizer=finbert_tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4368c340-094d-4460-b7a4-3de3c8ae1613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# Define trainning arguments \n",
    "training_args = TrainingArguments('FINBERT_TUTORIAL_MODEL', overwrite_output_dir=True, evaluation_strategy=\"steps\", \n",
    "                                  num_train_epochs=50, weight_decay=1e-8,learning_rate=1e-5,\n",
    "                                  eval_steps=50,metric_for_best_model='accuracy',\n",
    "                                 per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "                                 load_best_model_at_end = True, save_total_limit=10, save_steps=50,no_cuda=False,\n",
    "                             fp16=True,gradient_accumulation_steps=4)\n",
    "trainer = Trainer(\n",
    "    model =finbert_model, args=training_args, train_dataset=train_ds, eval_dataset=val_ds,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)], compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82bb6407-42b6-465e-b15e-603f7695b308",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3633\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 2850\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='2850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 450/2850 09:01 < 48:18, 0.83 it/s, Epoch 7/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.866877</td>\n",
       "      <td>0.655670</td>\n",
       "      <td>0.379775</td>\n",
       "      <td>0.385982</td>\n",
       "      <td>0.347946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.798958</td>\n",
       "      <td>0.661856</td>\n",
       "      <td>0.383019</td>\n",
       "      <td>0.405060</td>\n",
       "      <td>0.375458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.790812</td>\n",
       "      <td>0.668041</td>\n",
       "      <td>0.577104</td>\n",
       "      <td>0.438891</td>\n",
       "      <td>0.441481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.788355</td>\n",
       "      <td>0.661856</td>\n",
       "      <td>0.391183</td>\n",
       "      <td>0.444393</td>\n",
       "      <td>0.415665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.778862</td>\n",
       "      <td>0.668041</td>\n",
       "      <td>0.723798</td>\n",
       "      <td>0.436266</td>\n",
       "      <td>0.415665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.759319</td>\n",
       "      <td>0.686598</td>\n",
       "      <td>0.589647</td>\n",
       "      <td>0.521631</td>\n",
       "      <td>0.538242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.762198</td>\n",
       "      <td>0.682474</td>\n",
       "      <td>0.575031</td>\n",
       "      <td>0.497240</td>\n",
       "      <td>0.510482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.759441</td>\n",
       "      <td>0.674227</td>\n",
       "      <td>0.745806</td>\n",
       "      <td>0.489499</td>\n",
       "      <td>0.462450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.842594</td>\n",
       "      <td>0.632990</td>\n",
       "      <td>0.586049</td>\n",
       "      <td>0.521946</td>\n",
       "      <td>0.519378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to FINBERT_TUTORIAL_MODEL\\checkpoint-50\n",
      "Configuration saved in FINBERT_TUTORIAL_MODEL\\checkpoint-50\\config.json\n",
      "Model weights saved in FINBERT_TUTORIAL_MODEL\\checkpoint-50\\pytorch_model.bin\n",
      "Deleting older checkpoint [FINBERT_TUTORIAL_MODEL\\checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to FINBERT_TUTORIAL_MODEL\\checkpoint-100\n",
      "Configuration saved in FINBERT_TUTORIAL_MODEL\\checkpoint-100\\config.json\n",
      "Model weights saved in FINBERT_TUTORIAL_MODEL\\checkpoint-100\\pytorch_model.bin\n",
      "Deleting older checkpoint [FINBERT_TUTORIAL_MODEL\\checkpoint-250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to FINBERT_TUTORIAL_MODEL\\checkpoint-150\n",
      "Configuration saved in FINBERT_TUTORIAL_MODEL\\checkpoint-150\\config.json\n",
      "Model weights saved in FINBERT_TUTORIAL_MODEL\\checkpoint-150\\pytorch_model.bin\n",
      "Deleting older checkpoint [FINBERT_TUTORIAL_MODEL\\checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to FINBERT_TUTORIAL_MODEL\\checkpoint-200\n",
      "Configuration saved in FINBERT_TUTORIAL_MODEL\\checkpoint-200\\config.json\n",
      "Model weights saved in FINBERT_TUTORIAL_MODEL\\checkpoint-200\\pytorch_model.bin\n",
      "Deleting older checkpoint [FINBERT_TUTORIAL_MODEL\\checkpoint-350] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to FINBERT_TUTORIAL_MODEL\\checkpoint-250\n",
      "Configuration saved in FINBERT_TUTORIAL_MODEL\\checkpoint-250\\config.json\n",
      "Model weights saved in FINBERT_TUTORIAL_MODEL\\checkpoint-250\\pytorch_model.bin\n",
      "Deleting older checkpoint [FINBERT_TUTORIAL_MODEL\\checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to FINBERT_TUTORIAL_MODEL\\checkpoint-300\n",
      "Configuration saved in FINBERT_TUTORIAL_MODEL\\checkpoint-300\\config.json\n",
      "Model weights saved in FINBERT_TUTORIAL_MODEL\\checkpoint-300\\pytorch_model.bin\n",
      "Deleting older checkpoint [FINBERT_TUTORIAL_MODEL\\checkpoint-450] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to FINBERT_TUTORIAL_MODEL\\checkpoint-350\n",
      "Configuration saved in FINBERT_TUTORIAL_MODEL\\checkpoint-350\\config.json\n",
      "Model weights saved in FINBERT_TUTORIAL_MODEL\\checkpoint-350\\pytorch_model.bin\n",
      "Deleting older checkpoint [FINBERT_TUTORIAL_MODEL\\checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to FINBERT_TUTORIAL_MODEL\\checkpoint-400\n",
      "Configuration saved in FINBERT_TUTORIAL_MODEL\\checkpoint-400\\config.json\n",
      "Model weights saved in FINBERT_TUTORIAL_MODEL\\checkpoint-400\\pytorch_model.bin\n",
      "Deleting older checkpoint [FINBERT_TUTORIAL_MODEL\\checkpoint-550] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to FINBERT_TUTORIAL_MODEL\\checkpoint-450\n",
      "Configuration saved in FINBERT_TUTORIAL_MODEL\\checkpoint-450\\config.json\n",
      "Model weights saved in FINBERT_TUTORIAL_MODEL\\checkpoint-450\\pytorch_model.bin\n",
      "Deleting older checkpoint [FINBERT_TUTORIAL_MODEL\\checkpoint-600] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from FINBERT_TUTORIAL_MODEL\\checkpoint-300 (score: 0.6865979381443299).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=450, training_loss=0.7465708414713542, metrics={'train_runtime': 542.187, 'train_samples_per_second': 335.032, 'train_steps_per_second': 5.256, 'total_flos': 1887509880426240.0, 'train_loss': 0.7465708414713542, 'epoch': 7.89})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7195e22d-7a56-4ec3-a1d8-9487a1a55d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 727\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='92' max='46' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [46/46 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7873842716217041,\n",
       " 'eval_accuracy': 0.6905089408528198,\n",
       " 'eval_precision': 0.6117529178439484,\n",
       " 'eval_recall': 0.5410243125382639,\n",
       " 'eval_f1': 0.5588630727695495,\n",
       " 'eval_runtime': 6.5328,\n",
       " 'eval_samples_per_second': 111.285,\n",
       " 'eval_steps_per_second': 7.041,\n",
       " 'epoch': 7.89}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e8c8ad7-d2da-4bc8-a879-dcfb67e33125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 727\n",
      "  Batch size = 16\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4749925-a0ab-4567-9551-883ce58b1cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD4CAYAAADSIzzWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAeD0lEQVR4nO3deXxU1fnH8c+ThEXZZSeAKKJWrGJBxJ+iCCqIWrBUC7VK+1NjFVusSyvS1gVRa91qFX/SiuJWpGIVlaog4oJWBWVHICKVACaA7FRIMs/vj5nCoGEygSSHuXzfvM5rZs6999wn8+L15OTcc881d0dERKpfVugARET2V0rAIiKBKAGLiASiBCwiEogSsIhIIDlVfYLiNUs1zaKKtT3snNAh7BdWb90QOoTIK9m+wva2jYrknBpNDt3r8+2NKk/AIiLVKlYaOoK0KQGLSLR4LHQEaVMCFpFoiSkBi4gE4eoBi4gEUloSOoK0KQGLSLToIpyISCAaghARCUQX4UREwtBFOBGRUNQDFhEJpLQ4dARpUwIWkWjREISISCAaghARCUQ9YBGRQNQDFhEJw2O6CCciEkYG9YD1SCIRiRaPpV9SMLPaZvahmc02s/lmdkui/mYzW2FmsxKlb9Ixw8ws38wWmVnv8kJVD1hEoqXyFuPZBvR0981mVgN418z+mdh2n7vfnbyzmR0FDAQ6Aq2AKWZ2uLvvNiD1gEUkWiqpB+xxmxMfayRKqufN9QPGufs2d/8cyAe6pjqHErCIREsslnYxszwzm5FU8pKbMrNsM5sFFAGT3f2DxKarzGyOmY0xs0aJulxgedLhBYm63VICFpFoKS1Ju7j7aHfvklRGJzfl7qXu3gloDXQ1s6OBh4H2QCdgFXBPYveynrCc8gnNSsAiEi0V6AGny93XA9OAPu5emEjMMeAv7BxmKADaJB3WGliZql0lYBGJFPfStEsqZtbUzBom3h8AnA58amYtk3Y7D5iXeD8RGGhmtczsEKAD8GGqc2gWhIhES+XNA24JjDWzbOKd1fHu/rKZPWlmnYgPLywDLgdw9/lmNh5YAJQAQ1LNgAAlYBGJmkpaC8Ld5wDHlVF/UYpjRgIj0z2HErCIREsG3QmnBCwi0aLH0ouIBKLlKEVEAtEQhIhIIErA1Wfbtu0MHnI924uLKS0p5YzTTuaqS3e9SLlh4yZ+d8d9LF+xilo1azLixl/R4dB2e3Xe7du3M2zEPSxYtISGDepz963DyG3ZnE8Xf8aIux9k85atZGVnkXfxQM46/dS9Olcmq1WrJv+Y9AQ1a9UkJzuHlye+zt13PMg5/Xpz3Q1D6HDEofTt+SNmz5ofOtTIOPzw9jzz9MM7Ph96SFtuvuVuHvjzXwNGVY00BFF9ataswZgH7uTAAw+guKSEi6+4ju7dunDs0d/Zsc9fnniWIzu054E7fs/Sfy9n5D0P8egDd6bV/opVhQwfeQ+PP3jXLvXPv/w69evV5Z/jxzBpyjTuHTWGe0YMo3btWtz+u+s4uE0uRavXcsElv+CkEzpTv17dSv25M8W2bdv54ff/l61btpKTk8OLrz7F1Mlvs2jhEi656Jfcdf/NoUOMnMWLP6PL8WcCkJWVxRfLZvLCi/8s56gIyaCLcBl/J5yZceCBBwBQUlJCSUkJZrvekv3Zsi/o1vlYAA49uA0rVhWy5qt1ALz02lQGXjqUAYOHcMtdD1Bamt5SdlPfeZ9+fU8H4Mwe3flg5izcnXZtW3Nwm/j6G82aNuagRg1Zt35DpfysmWrrlq0A1KiRQ40aObjDksVL+Sx/WdjA9gO9ep7M0qX/5osvVoQOpfpUwa3IVSXjEzBAaWkpAwYP4ZRzBnHi8cdxTMcjd9l+xGGHMuWt9wCYu2ARqwqLKCxaw2fLvuDVN97iyf+7hwljHyIrK4uXX38zrXMWrV5Li2ZNAMjJyaZunQNZv2HjLvvMXbCI4uIS2uS2LKuJ/UZWVhaT33meuUve5a033+OTmXNCh7TfuOCCfox79oXQYVSvSlqOsjqUOwRhZkcSX+cyl/itdyuBie6+sIpjS1t2djYTxj7Exk2bGTpsBEuWLttljPfSi87nzvsfYcDgIXRo344jO7QnOzubD2bMYsGn+Qy8ZCgA27Zt46BGDQH45bBbWbGykOKSYlYVrmbA4CEA/OSCfpx39pm4f3uRo+Se9+o1XzHs1j8y8rfXkpUVid9zeywWi3FG9x9Qv0E9xjz1AEd85zAWLcwPHVbk1ahRg3PPOZPhv70jdCjVax/o2aYrZQI2s98Ag4Bx7FxUojXwNzMb5+5lDqQm1tTMAxh1z21cevGgyos4hfr16nL8947h3X/N2CUB161Th9uGXwOAu9P7hz+ldavmzJw1l++fdTq/uuJn32rrgTt+D+x+DLh5syZ8WbSGFs2aUlJSyuYtW2lQvx4Am7ds4crrf88v8gbvMha9v9u4YRPvvfsRp/XqrgRcDfr0OY1PPplLUdGa0KFUrwxKwOV1zS4Bjnf3O939qUS5k/jya5fs7qDkNTarOvl+tW49GzfFF63/ets2/vXRJxxycJtd9tm4aTPFxfEnpU546VU6d/oudevUoVuXTkye9i5r160H4rMlVn5ZmNZ5Tzu5Gy9OmgLA69Pe4YTOx2JmFBcXM3TYCL7fpxe9e3avpJ8yczVu3Ij6DeK/mGrXrsUpp55I/pKlgaPaPwz8Uf/9b/gBwD39Elh5QxAx4s82+vc36lsmtgW3eu06ht92N6WxGB5zevfsTo+TTuDZf7wCwI/OO5ul/17OjSPuJjsri0PbteXWYVcD0P6Qg/nFZReTd/VwYh6jRk4Ow6+5klYtmpd73h+c05thI/7IWRf8Lw3q1+OPt9wAwKtT32HmrHms37CJFxIJeuTwazjy8PZV8wXs45q1aMqfHr6D7OwssiyLiS+8ypTX3uKsc3px2x+G07jJQTw5/mHmz/2UQQPyym9Q0nLAAbU5vdcpXHHlb0KHUv1KMmcWhJU1lrljo1kf4EFgCTsftdEWOAy4yt1fLe8ExWuWhv81E3FtDzsndAj7hdVb9+/ZLNWhZPuKsp4qUSH/eWp42jnngJ+M3Ovz7Y2UPWB3f9XMDic+5JBL/JEbBcBH5a1zKSISRAaNAZc7CyLx2I1/VUMsIiJ7bx8Y201Xxt8JJyKyiyj1gEVEMooSsIhIGJ7mcgL7AiVgEYmWDOoB79/3yIpI9FTSWhBmVtvMPjSz2WY238xuSdQfZGaTzWxJ4rVR0jHDzCzfzBaZWe/yQlUCFpFoiXn6JbVtQE93PxboBPQxs27ADcAb7t4BeCPxGTM7ChgIdAT6AKMSj7TfLSVgEYmWSlqO0uM2Jz7WSBQnvjjZ2ET9WKB/4n0/YJy7b3P3z4F84vdQ7JYSsIhES2lp2sXM8sxsRlLZ5X54M8s2s1lAETDZ3T8Amrv7KoDEa7PE7rnsvGMY4jet5aYKVRfhRCRaKnARzt1HA6NTbC8FOplZQ+AfZnZ0iubKuq055TiHesAiEi2VNwa8g7uvB6YRH9stNLOWAInXosRuBUDyUoytia+fvltKwCISLZU3C6JpoueLmR0AnA58CkwEBid2Gwy8mHg/ERhoZrXM7BCgAzvXUS+ThiBEJFoq0LMtR0tgbGImQxYw3t1fNrP3gfFmdgnwBXA+gLvPN7PxwAKgBBhS3qJlSsAiEileSTdiuPsc4Lgy6tcCvXZzzEhgZLrnUAIWkWjRrcgiIoFU3hBElVMCFpFoyaC1IJSARSRa1AMWEQmknOll+xIlYBGJFvWARUTC8BLNghARCUM9YBGRQDQGLCISiHrAIiJhuBKwiEgguggnIhKIesAiIoEoAYuIhOGuBCwiEoZ6wCIigSgB73RFl19X9Sn2eyc36BA6hP3Ca8VzQ4cgafAS3YghIhJG5uRfPRVZRKLFY552ScXM2pjZm2a20Mzmm9nQRP3NZrbCzGYlSt+kY4aZWb6ZLTKz3uXFqh6wiERL5Y0BlwDXuvvHZlYPmGlmkxPb7nP3u5N3NrOjgIFAR6AVMMXMDk/1ZGT1gEUkWmIVKCm4+yp3/zjxfhOwEMhNcUg/YJy7b3P3z4F8oGuqcygBi0ikVGQIwszyzGxGUskrq00za0f8EfUfJKquMrM5ZjbGzBol6nKB5UmHFZA6YSsBi0i0eImnX9xHu3uXpDL6m+2ZWV1gAnC1u28EHgbaA52AVcA9/921rHBSxaoxYBGJlkqcBWFmNYgn36fd/XkAdy9M2v4X4OXExwKgTdLhrYGVqdpXD1hEIsVj6ZdUzMyAR4GF7n5vUn3LpN3OA+Yl3k8EBppZLTM7BOgAfJjqHOoBi0i0VF4P+CTgImCumc1K1N0IDDKzTsSHF5YBlwO4+3wzGw8sID6DYkiqGRCgBCwiEVNZTyRy93cpe1x3UopjRgIj0z2HErCIRIqXhI4gfUrAIhIpGfRMTiVgEYkWJWARkVC8rGHbfZMSsIhEinrAIiKBeEw9YBGRIGKlSsAiIkFoCEJEJBANQYiIBJJBT6VXAhaRaFEPWEQkEF2EExEJRD1gEZFAXHfCiYiEoWloIiKBxNQDFhEJQ0MQIiKBaBaEiEggmgUhIhJIJo0B67H0IhIp7pZ2ScXM2pjZm2a20Mzmm9nQRP1BZjbZzJYkXhslHTPMzPLNbJGZ9S4v1kj2gH9615Uc07Mzm9Zu4Kbe1+x1e/8z4FTOvuqHALzy4HO8N+EtAC69fyjtvnsopSWlfD47nydvfITSkpRPod4v9f3ZOfQadCZmxpS/vc6kMS9Rt0FdfvXQ9TRt3YzVBUXce+VdbNm4JXSoGSs3tyWP/OVumjdvSiwW4/HHxvHwqMdp1KgBjz3xZw5u25p/f1HATy+6ivXrN4YOt0pV4loQJcC17v6xmdUDZprZZOCnwBvufqeZ3QDcAPzGzI4CBgIdgVbAFDM7PNWj6SPZA57+3JvcP/i2Ch93/bhbaNy66S51dRrU5dyhF3B7/2GM7HcD5w69gAPr1wHggxfe5re9hnJT72uoUbsm3Qf2qpT4o6TN4W3pNehMhn3/Oq7rM5TOvY6nRbuW9L9yAHOnz+GXPa5g7vQ59L9yQOhQM1pJaQnDb7yd4zufSa/TBnBZ3kUcceRh/Oran/PWtPc47tievDXtPX517RWhQ61yMbe0SyruvsrdP0683wQsBHKBfsDYxG5jgf6J9/2Ace6+zd0/B/KBrqnOEckEvOTDhWzZsHmXuqZtm3P12OH87qU/8OvxI2jRvlVabXU89VgWvDubLRs2s3XjFha8O5uje3QCYO60T3bst2x2Po1aNK60nyEqcg9rzZJPFrP96+3ESmMs+GAeXXt34/gzTmDahKkATJswla5ndgscaWYr/HI1s2fNB2Dz5i0sWpRPq1YtOPvsM3jm6QkAPPP0BM4554yQYVaLWMzSLmaWZ2YzkkpeWW2aWTvgOOADoLm7r4J4kgaaJXbLBZYnHVaQqNutSCbgslx8x8955qZHGXHub/j77U9w4YjL0jquYfPGfLVy7Y7P61Z9RcPmuyba7Jxsup13CvPemlWZIUfC8sVf8J2uR1G3YT1q1q7J907rTJNWTWjQpAHri9YBsL5oHfWbNAgcaXS0bZvLMcd2ZMZHs2jarAmFX64G4km6SdPodxIq0gN299Hu3iWpjP5me2ZWF5gAXO3uqcZvyupSpxwQ2eMxYDP7mbs/tptteUAewEkHHceR9Q7d09NUiloH1qZ958P5+ahrd9TVqFkDgJPOP41eP+sLQLODWzD0sRspKS5hzfIiRl3+R6zMr3TX7/TCEZex+MMFLPloYZX9DJlqRX4BL/7f8/zu6Vv4esvXLFuwjNKSDLpXNMPUqXMgTz4ziht+PYJNmzaXf0AEVeaNGGZWg3jyfdrdn09UF5pZS3dfZWYtgaJEfQHQJunw1sDKVO3vzUW4W4AyE3Dit8hogEvb/TD48siWZWzduJVb+17/rW3T//4m0//+JhAfAx5z3YOsLVi9Y/u6L9dyRLeOOz43ankQi/41f8fnc4eeT73G9Xny8keq8CfIbFOfncLUZ6cAMOj6n7D2y7VsWLOBhs0asb5oHQ2bNWLjmg2Bo8x8OTk5PPXMKMY/O5GXJr4GwOqiNTRv0ZTCL1fTvEVT1qxeW04rma+ypqGZmQGPAgvd/d6kTROBwcCdidcXk+qfMbN7iV+E6wB8mOocKYcgzGzObspcoPke/VQBfL35P6xZXkTnvifuqGv9nYPTOnb+W7M5qvuxHFi/DgfWr8NR3Y9l/luzAej+o150PKUTo39xP55Jy/BXs/qN48MLTVo14YQ+JzL9xbeZMeVDegzoCUCPAT35aPIHIUOMhIcevpNFiz7joT8/uqNu0qQp/PjC+AXOH184gFdemRwqvGrjFSjlOAm4COhpZrMSpS/xxHuGmS0Bzkh8xt3nA+OBBcCrwJBUMyAALFXiMLNCoDew7pubgPfcvdwrWSF6wJc9cDVHdOtI3Ub12LhmAxPve5ZP35vHT0ZeRoNmjcjOyebDl6bz8gPP7XJcWT1ggJPO78nZQ84D4JWHnt/RY34k/1nWrljN11v+A8DHr37wrTarwwYvrvZzVsStf7+deo3qU1JcwtjbxjBv+hzqNqzHNaOup0mrpqxZuZp7r7iLzRv27T+ZX1szN3QIu9XtxC68PmU88+Z9SiwWH+K59ea7mfHRLB5/8kHatG7F8oKVDP7JENat23f/2ti4Zeled1+nt0g/55z05XNB79ooLwE/Cjzm7u+Wse0Zd/9xeSfYF4Ygom5fT8BRsS8n4KiojAT8TgUScPfACTjlGLC7X5JiW7nJV0SkunmZkxH2TZG8E05E9l+xDPqbWwlYRCIlph6wiEgYGoIQEQmkVAlYRCSMTLrPUglYRCJFCVhEJBCNAYuIBJJBj4RTAhaRaNE0NBGRQDLpoWBKwCISKbEyF/HeNykBi0ikZNCdyErAIhItmoYmIhKIZkGIiASiW5FFRAJRD1hEJBCNAYuIBJJJsyBSPhVZRCTTxCz9Uh4zG2NmRWY2L6nuZjNb8Y0nJf932zAzyzezRWbWu7z2lYBFJFJiFShpeBzoU0b9fe7eKVEmAZjZUcBAoGPimFFmlp2qcSVgEYmUUku/lMfd3wa+SvPU/YBx7r7N3T8H8oGuqQ5QAhaRSKlID9jM8sxsRlLJS/M0V5nZnMQQRaNEXS6wPGmfgkTdbikBi0ikVCQBu/tod++SVEancYqHgfZAJ2AVcE+ivqw+dcprgkrAIhIpXoGyR+27F7p7qbvHgL+wc5ihAGiTtGtrYGWqtpSARSRSKnMWRFnMrGXSx/OA/86QmAgMNLNaZnYI0AH4MFVbmgcsIpFSmTdimNnfgB5AEzMrAG4CephZJ+Kd6GXA5QDuPt/MxgMLgBJgiLunXJ5YCVhEIqUyF2R390FlVD+aYv+RwMh021cCFpFI0VoQIiKBaC0IEZFAMmktiCpPwJM3La7qU+z3thR/HTqE/ULNbPVXMkEsg1Kw/keJSKToqcgiIoFoDFhEJBDNghARCURjwCIigWRO+lUCFpGI0RiwiEggpRnUB1YCFpFIUQ9YRCQQXYQTEQkkc9KvErCIRIyGIEREAtFFOBGRQDQGLCISSOakXyVgEYmYTOoB66nIIhIpsQqU8pjZGDMrMrN5SXUHmdlkM1uSeG2UtG2YmeWb2SIz611e+0rAIhIpXoF/aXgc6PONuhuAN9y9A/BG4jNmdhQwEOiYOGaUmWWnalwJWEQipRRPu5TH3d8GvvpGdT9gbOL9WKB/Uv04d9/m7p8D+UDXVO0rAYtIpFRkCMLM8sxsRlLJS+MUzd19FUDitVmiPhdYnrRfQaJut3QRTkQiJebpX4Rz99HA6Eo6dVlLwacMRj1gEYkUr0DZQ4Vm1hIg8VqUqC8A2iTt1xpYmaohJWARiZQYnnbZQxOBwYn3g4EXk+oHmlktMzsE6AB8mKohDUGISKSkObshLWb2N6AH0MTMCoCbgDuB8WZ2CfAFcD6Au883s/HAAqAEGOLuKR/SrAQsIpFSUokJ2N0H7WZTr93sPxIYmW77SsAiEimV2QOuakrAIhIpWo5SRCQQr8A0tNCUgEUkUjJpMR4lYBGJFC3ILiISiHrAIiKBaAw4Q9SqVZPxLz9GzZo1ycnJZtLEKdz3h1E8+Ne7OPSwdgDUb1CPjRs20bfHBWGDzVCtclsw6pG7aNa8KbFYjCcef5bRDz8BwKWXX8SleRdSUlLK5Nemccvv/xg42syl73knzYLIENu2bWdQ/0vZuuU/5OTk8NyksUx7412uuvTXO/b57a3XsnHj5oBRZrbSklJ+P/xO5sxeQN26dXjj7eeZNnU6zZo14ay+vTjlxHPZvr2YJk0OCh1qRtP3vJPmAWeQrVv+A0BOjRxq5OR868+Xs/v3ZlD/S0OEFgmFhaspLFwNwObNW1i86DNatmrORYMv4E/3jWb79mIA1qz55pKrUhH6nnfKpDHg/X4xnqysLCZNG8/Hn07jnbfeZ9bMuTu2dT2xM2tWr2XZ0i8CRhgdbdrm8t1jjmLmjNm0P+wQTvyfLrw29e9MnPQUx33vu6HDi4z9/Xsu9VjaJbRyE7CZHWlmvcys7jfqv/mYjowUi8Xo2+MCun33DDoddzSHH3nYjm3fH3AWEyf8M2B00VGnzoE8/uSfGX7D7WzetIWcnGwaNKxP757nc9Pv7uKvj98fOsRI0Pdc6Y8kqlIpE7CZ/ZL4Umu/AOaZWb+kzbenOG7HKvObv86MP3k2btzE+9Nn0KPXSQBkZ2fT5+xevPTCa4Ejy3w5OTk89tSfeW78S7zy0usArFz5Ja9MjL//ZOYcYu40btwoVTNSDn3PcTH3tEto5fWALwM6u3t/4kuy/c7Mhia2lbX6OxBfZd7du7h7l7q1991B/4MaN6J+/XoA1Kpdi5NP7Ub+ks8BOPnUbny25HO+XFkYMsRI+NNDt7N40Wc8/NBjO+r++fIUup/aDYD2h7WjZo0arF27LlSIkaDvOa4aFmSvNOVdhMt2980A7r7MzHoAz5nZwaRIwJmiWfMm3PvQbWRlZ5OVlcXLL7zG1NffBuDcH/Rh4vMafthbJ3TrzI8G9Wf+vE958934utUjb72Xp5+cwAOjbuedf71M8fZirvr5bwJHmtn0Pe+USRfhLNWkZTObClzj7rOS6nKAMcCF7p7ykcsABzc+JnO+jQy1pfjr0CGIVIo1GxfvdcfuxNzT0s457694M2hHsrwe8MXEV3bfwd1LgIvN7JEqi0pEZA/tC7Mb0pUyAbt7QYpt0ys/HBGRvbMvzG5I135/I4aIRIvWghARCSSTLsIpAYtIpFRmD9jMlgGbgFKgxN27mNlBwLNAO2AZcIG779Hcvv3+VmQRiZZSYmmXNJ3m7p3cvUvi8w3AG+7eAXgj8XmPKAGLSKRUw51w/YCxifdjgf572pASsIhESkXWgkheNiFR8r7VHLxuZjOTtjV391UAiddmexqrxoBFJFIq0rN199HA6BS7nOTuK82sGTDZzD7d2/iSqQcsIpFSmauhufvKxGsR8A+gK1BoZi0BEq9FexqrErCIREpljQGbWR0zq/ff98CZwDxgIjA4sdtg4itG7hENQYhIpFTircjNgX+YGcRz5TPu/qqZfQSMN7NLgC+A8/f0BErAIhIplXUrsrsvBY4to34t0KsyzqEELCKR4lFZjEdEJNPoVmQRkUC0GI+ISCDqAYuIBFIa0xiwiEgQWpBdRCQQjQGLiASiMWARkUDUAxYRCUQX4UREAtEQhIhIIBqCEBEJZC8eNVTtlIBFJFI0D1hEJBD1gEVEAolpOUoRkTB0EU5EJBAlYBGRQDIn/YJl0m+L6mJmee4+OnQcUabvuOrpO9736bH0ZcsLHcB+QN9x1dN3vI9TAhYRCUQJWEQkECXgsmncrOrpO656+o73cboIJyISiHrAIiKBKAGLiASiBJzEzPqY2SIzyzezG0LHE0VmNsbMisxsXuhYosrM2pjZm2a20Mzmm9nQ0DFJ2TQGnGBm2cBi4AygAPgIGOTuC4IGFjFmdgqwGXjC3Y8OHU8UmVlLoKW7f2xm9YCZQH/9X973qAe8U1cg392Xuvt2YBzQL3BMkePubwNfhY4jytx9lbt/nHi/CVgI5IaNSsqiBLxTLrA86XMB+k8rGc7M2gHHAR8EDkXKoAS8k5VRp/EZyVhmVheYAFzt7htDxyPfpgS8UwHQJulza2BloFhE9oqZ1SCefJ929+dDxyNlUwLe6SOgg5kdYmY1gYHAxMAxiVSYmRnwKLDQ3e8NHY/snhJwgruXAFcBrxG/aDHe3eeHjSp6zOxvwPvAEWZWYGaXhI4pgk4CLgJ6mtmsROkbOij5Nk1DExEJRD1gEZFAlIBFRAJRAhYRCUQJWEQkECVgEZFAlIBFRAJRAhYRCeT/AWjDPWQIM2MJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = np.argmax(predictions.predictions,1)\n",
    "sns.heatmap(confusion_matrix(test.label.values,output),annot =confusion_matrix(test.label.values,output))#,labels = ['1','-1','0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88903b98",
   "metadata": {},
   "source": [
    "## 4. Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "abe53179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers_interpret import SequenceClassificationExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6847a80",
   "metadata": {},
   "source": [
    "To visualise which words in each phrase are the most important for the prediction we will use the python package transformers_interpret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7cd84b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_model_name = \"ProsusAI/finbert\"\n",
    "model_name = \"textattack/bert-base-uncased-SST-2\"\n",
    "\n",
    "\n",
    "fin_model = AutoModelForSequenceClassification.from_pretrained(fin_model_name)\n",
    "fin_tokenizer = AutoTokenizer.from_pretrained(fin_model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "be06563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With both the model and tokenizer initialized we are now able to get explanations on an example text.\n",
    "cls_explainer = SequenceClassificationExplainer(model,\n",
    "                                                tokenizer)\n",
    "\n",
    "fin_cls_explainer = SequenceClassificationExplainer(fin_model,\n",
    "                                                    fin_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9979f669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0ff99d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_attributions = cls_explainer(\"Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&D and marketing\")\n",
    "word_attributions = fin_cls_explainer(\"Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&D and marketing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf21c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LABEL_0'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_explainer.predicted_class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4a6dda98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0 (0.99)</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0</b></text></td><td><text style=\"padding-right:2em\"><b>1.37</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pharmaceuticals                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> group                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> orion                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> corp                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> reported                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fall                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> its                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> third                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> -                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> quarter                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> earnings                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hit                    </font></mark><mark style=\"background-color: hsl(120, 75%, 65%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> larger                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> expenditures                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> r                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> &                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> d                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> marketing                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bert_vis = cls_explainer.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "050d7ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>negative (0.98)</b></text></td><td><text style=\"padding-right:2em\"><b>negative</b></text></td><td><text style=\"padding-right:2em\"><b>2.61</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pharmaceuticals                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> group                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> orion                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> corp                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> reported                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fall                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> its                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> third                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> -                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> quarter                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> earnings                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hit                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> larger                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> expenditures                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> r                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> &                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> d                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> marketing                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fin_bert_vis = fin_cls_explainer.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e72ac68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f723887c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
