{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ae4cd8",
   "metadata": {},
   "source": [
    "# Oxford Man Institute NLP Tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d7c68",
   "metadata": {},
   "source": [
    "## 1. Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e5b5a0",
   "metadata": {},
   "source": [
    "There are several ways of performing sentiment classification on a document or article, ranging from word-counts to modern Transformer-based Language Models. In this tutorial we will take you through a range of classification techniques:\n",
    "- Loughran & McDonald financial sentiment dictionary\n",
    "- Naive Bayes Classifier\n",
    "- BERT out of the box\n",
    "- BERT fine-tuned on general sentiment datasets\n",
    "- FinBERT \n",
    "    - BERT that has been trained on positive and negative financial documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0879d0bc",
   "metadata": {},
   "source": [
    "## 2. Traditional sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272fa01e",
   "metadata": {},
   "source": [
    "### Import packages and load dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c3d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d35271",
   "metadata": {},
   "source": [
    "### Loughran & McDonald classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5446979d-60e4-4d7a-9597-d608a36170a0",
   "metadata": {},
   "source": [
    "Loughran & McDonald released their master dictionary in 2011 in conjunction with their paper â€œWhen is a Liability not a Liability? Textual Analysis, Dictionaries, and 10-Ks\". The dictionary lists a number of words and includes negative, positive, uncertainty, litigious, strong modal, weak modal, and constraining tags. \n",
    "\n",
    "There are several shortcomings to this simplistic approach:\n",
    "\n",
    "- **Some words don't appear in the dictionary (fall, rise, etc.)**\n",
    "- **Some words are negative/positive given the context they are written (profit, expenditure, etc.)**\n",
    "- **Simple counts of words don't necessarily infer the overall sentiment**\n",
    "    - *Hatred for football has always confused me; there are so many haters who attack the sport, but I have always loved it.* - 3 negative words and 1 positive word.\n",
    "\n",
    "\n",
    "We have taken the words that have a negative and positive tag for our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "608eae79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some examples of negative words:  ['abandon', 'abandoned', 'abandoning', 'abandonment', 'abandonments']\n",
      "Some examples of positive words:  ['able', 'abundance', 'abundant', 'acclaimed', 'accomplish']\n"
     ]
    }
   ],
   "source": [
    "lmdict = np.load('data/LoughranMcDonald_dict.npy', allow_pickle='TRUE').item()\n",
    "print('Some examples of negative words: ', lmdict['Negative'][:5])\n",
    "print('Some examples of positive words: ', lmdict['Positive'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "441f95e2-be4a-4263-96da-9dc07d68a45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able', 'abundance', 'abundant', 'acclaimed', 'accomplish']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmdict['Positive'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00495202-583a-4b11-8ede-7cfb61fd41db",
   "metadata": {},
   "source": [
    "Check to see if a word appears in the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "004cd19d-8fbe-4e5c-9538-2cfc0017d73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, fall is not in the Loughran & McDonald dictionary\n"
     ]
    }
   ],
   "source": [
    "word = 'fall'\n",
    "\n",
    "if word in lmdict['Negative']:\n",
    "    print(f'Yes, {word} is a Negative word in the Loughran & McDonald dictionary')\n",
    "elif word in lmdict['Positive']:\n",
    "    print(f'Yes, {word} is Positive word in the Loughran & McDonald dictionary')\n",
    "else:\n",
    "    print(f'No, {word} is not in the Loughran & McDonald dictionary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f1465",
   "metadata": {},
   "source": [
    "Negation is another challenge that emerges using this approach. A techy fix is to check if the word is preceeded by a negating word in our list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48d384c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "negate = [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\", \"ain't\", \"aren't\", \"can't\",\n",
    "          \"couldn't\", \"daren't\", \"didn't\", \"doesn't\", \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\",\n",
    "          \"neither\", \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\", \"neednt\", \"needn't\",\n",
    "          \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\", \"oughtnt\", \"shant\", \"shouldnt\", \"wasnt\",\n",
    "          \"werent\", \"oughtn't\", \"shan't\", \"shouldn't\", \"wasn't\", \"weren't\", \"without\", \"wont\", \"wouldnt\", \"won't\",\n",
    "          \"wouldn't\", \"rarely\", \"seldom\", \"despite\", \"no\", \"nobody\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a0377ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def negated(word):\n",
    "    \"\"\"\n",
    "    Determine if preceding word is a negation word\n",
    "    \"\"\"\n",
    "    if word.lower() in negate:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcd91aa",
   "metadata": {},
   "source": [
    "This function counts the number of negative and positive words in a document and performs a negation check to switch the polarity of words that are preceeded by a word in the *negate* list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ec407be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results with negation check:\n",
      "\n",
      "The # of positive words: 0\n",
      "The # of negative words: 2\n",
      "The list of found positive words: []\n",
      "The list of found negative words: ['able (with negation)', 'abandon']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[26, 0, 2, [], ['able (with negation)', 'abandon']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tone_count_with_negation_check(dict, article):\n",
    "    \"\"\"\n",
    "    Count positive and negative words with negation check. Account for simple negation only for positive words.\n",
    "    Simple negation is taken to be observations of one of negate words occurring within three words\n",
    "    preceding a positive words.\n",
    "    \"\"\"\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    " \n",
    "    pos_words = []\n",
    "    neg_words = []\n",
    " \n",
    "    input_words = re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', article.lower())\n",
    " \n",
    "    word_count = len(input_words)\n",
    " \n",
    "    for i in range(0, word_count):\n",
    "        if input_words[i] in dict['Negative']:\n",
    "            neg_count += 1\n",
    "            neg_words.append(input_words[i])\n",
    "        if input_words[i] in dict['Positive']:\n",
    "            if i >= 3:\n",
    "                if negated(input_words[i - 1]) or negated(input_words[i - 2]) or negated(input_words[i - 3]):\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i])\n",
    "            elif i == 2:\n",
    "                if negated(input_words[i - 1]) or negated(input_words[i - 2]):\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i])\n",
    "            elif i == 1:\n",
    "                if negated(input_words[i - 1]):\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i])\n",
    "            elif i == 0:\n",
    "                pos_count += 1\n",
    "                pos_words.append(input_words[i])\n",
    " \n",
    "    print('The results with negation check:', end='\\n\\n')\n",
    "    print('The # of positive words:', pos_count)\n",
    "    print('The # of negative words:', neg_count)\n",
    "    print('The list of found positive words:', pos_words)\n",
    "    print('The list of found negative words:', neg_words)\n",
    "    print('\\n', end='')\n",
    " \n",
    "    results = [word_count, pos_count, neg_count, pos_words, neg_words]\n",
    " \n",
    "    return results\n",
    " \n",
    "    \n",
    "# A sample output\n",
    "article = '''Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&D and marketing not able abandon'''\n",
    " \n",
    "tone_count_with_negation_check(lmdict, article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ea5bc-68f8-41fb-8c08-bea0b0a26289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f930574",
   "metadata": {},
   "source": [
    "## 3. BERT classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03516dc1-abee-498f-a856-804b47983c23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import all dependencies \n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import  TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from transformers import AutoConfig\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91227355-8b5c-4bb6-bcc8-2485bdb4949c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the dataset from huggingfaces' dataset repository\n",
    "#fin_dataset = load_dataset('financial_phrasebank', 'sentences_allagree')\n",
    "#df = pd.DataFrame(fin_dataset['train']) # send  it to a pandas dataframe\n",
    "\n",
    "# HUGE INSTALL PROBLEM ON WINDOWS NEED TO FIX - RUNS FINE ON MAC - \n",
    "\n",
    "# MEANTIME INSTALL FROM data/FinancialPhraseBanl-v1.0\n",
    "\n",
    "df = pd.read_csv('data\\\\FinancialPhraseBank-v1.0\\\\Sentences_50Agree.txt',\n",
    "            encoding = 'ISO-8859-1',on_bad_lines='skip',sep = '.@')\n",
    "df.columns = ['sentence','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9054771-2667-4ab2-9d5f-162dee478a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].replace(to_replace=({'neutral':0,'positive':1,'negative':2}))\n",
    "df['label']= df['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "228a2858",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22345d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "675e1335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is our input sentence : \n",
      " Hi my name is BERT and I am overjoyed  to meet you ! \n",
      "\n",
      "These are the outputs of the tokenizer:\n",
      "\n",
      "{'input_ids': tensor([[  101,  8790,  1139,  1271,  1110,   139,  9637,  1942,  1105,   146,\n",
      "          1821,  1166, 18734,  1174,  1106,  2283,  1128,   106,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "\n",
      "These inputs correspond to the original sentence with separation and padding thrown in :\n",
      "\n",
      "['[CLS] Hi my name is BERT and I am overjoyed to meet you! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n"
     ]
    }
   ],
   "source": [
    "#how does the tokenizer work ? \n",
    "print('\\nThis is our input sentence : \\n Hi my name is BERT and I am overjoyed  to meet you ! \\n')\n",
    "\n",
    "out = tokenizer(['Hi my name is BERT and I am overjoyed  to meet you ! '],\n",
    "          max_length=64,padding=\"max_length\", truncation=True,return_tensors='pt')\n",
    "print('These are the outputs of the tokenizer:\\n')\n",
    "print(out)\n",
    "\n",
    "print('\\nThese inputs correspond to the original sentence with separation and padding thrown in :\\n')\n",
    "print([tokenizer.decode(i) for i in out['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ed749a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is our model : \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " First layer shape (vocabulary size) : \n",
      "  torch.Size([28996, 768]) \n",
      " Last layer shape (prediction task output shape) : \n",
      "  torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Now that we covered  the tokenizer lets introduce the other building block : the model \n",
    "\n",
    "print('this is our model : \\n')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased')\n",
    "layers = [i for i in model.parameters()]\n",
    "print('\\n First layer shape (vocabulary size) : \\n ',layers[0].shape,\n",
    "'\\n Last layer shape (prediction task output shape) : \\n ',layers[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9c745e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is our forward propagation syntax. \n",
      " We feed in a tokenized text and receive the \n",
      " predicted  logits over the 2 classes : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.6428,  0.3938]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic forward propagation of our BERT model \n",
    "print('This is our forward propagation syntax. \\n We feed in a tokenized text and receive the \\n predicted  logits over the 2 classes : \\n')\n",
    "model.forward(**out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebec1e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with BERT hands-on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baf53dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  define tokenizer & model \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# turn the configuration for a 3 sentiment classification task\n",
    "config = AutoConfig.from_pretrained('bert-base-cased')\n",
    "config.num_labels = 3\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_config(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "966ed0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.25, random_state=96)\n",
    "test, val = train_test_split(test, test_size=0.4, random_state=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38988d-5ed7-4eaa-9c28-6c6ddef1ba4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "423f2be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a Dataset object to put our data in\n",
    "\n",
    "\n",
    "class BERTTutorialDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Special dataset class built on top of the torch Dataset class\n",
    "    useful to have memory efficient dataloading tokenization batching and trainning.\n",
    "    \n",
    "    Huggingface can use these types of dataset as inputs and run all trainning/prediction on them. \n",
    "    \"\"\"\n",
    "    def __init__(self, input_data, sentiment_targets, tokenizer, max_len):\n",
    "        \"\"\"\n",
    "        Basic generator function for the class.\n",
    "        -----------------\n",
    "        input_data : array\n",
    "            Numpy array of string  input text to use for downstream task \n",
    "        sentiment_targets : \n",
    "            Numpy array of integers indexed in  the pytorch style of [0,C-1] with C being the total number of classes\n",
    "            In our example this means the target sentiments should range from 0 to 2. \n",
    "        tokenizer  : Huggingface tokenizer \n",
    "            The huggingface tokenizer to use\n",
    "        max_len : \n",
    "            The truncation length of the tokenizer \n",
    "        -------------------\n",
    "        \n",
    "        Returns : \n",
    "        \n",
    "            Tokenized text with inputs, attentions and labels, ready for the Training script. \n",
    "        \"\"\"\n",
    "        self.input_data = input_data\n",
    "        self.sentiment_targets = sentiment_targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Function required by torch huggingface to batch efficiently\n",
    "        \"\"\"\n",
    "        return len(self.input_data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.input_data[item])\n",
    "        target = self.sentiment_targets[item]\n",
    "        # only difference with the previuous tokenization step is the encode-plus for special tokens\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          text,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          return_token_type_ids=False,\n",
    "          padding='max_length',\n",
    "          return_attention_mask=True,\n",
    "          return_tensors='pt',\n",
    "          truncation = True\n",
    "        )\n",
    "        return {\n",
    "          'text': text,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'labels': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a52df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our train-val-test datasets\n",
    "MAX_LEN = 128\n",
    "train_ds = BERTTutorialDataset(\n",
    "    input_data=train['sentence'].to_numpy(),\n",
    "        sentiment_targets=train['label'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "val_ds = BERTTutorialDataset(\n",
    "    input_data=val['sentence'].to_numpy(),\n",
    "        sentiment_targets=val['label'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "\n",
    "test_ds = BERTTutorialDataset(\n",
    "    input_data=test['sentence'].to_numpy(),\n",
    "        sentiment_targets=test['label'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfc2a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some accuracy measure ( helpful for the early stopping )\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import numpy as np\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    Function to calculate accuracies and losses for the validation from the predicted outputs\n",
    "    This is neccessary for the early stopping. \n",
    "    \"\"\"\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred, average='macro')\n",
    "    precision = precision_score(y_true=labels, y_pred=pred, average='macro')\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred, average='macro')    \n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71b300ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# Define trainning arguments \n",
    "training_args = TrainingArguments('BERT_TUTORIAL_MODEL', overwrite_output_dir=True, evaluation_strategy=\"steps\", \n",
    "                                  num_train_epochs=50, weight_decay=0.005,learning_rate=1e-5,\n",
    "                                  eval_steps=200,metric_for_best_model='accuracy',\n",
    "                                 per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "                                 load_best_model_at_end = True, save_total_limit=10, save_steps=200,no_cuda=False,\n",
    "                             fp16=True,gradient_accumulation_steps=4)\n",
    "trainer = Trainer(\n",
    "    model =pretrained, args=training_args, train_dataset=train_ds, eval_dataset=val_ds,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=20)], compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf13f2e4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3633\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 2850\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1601' max='2850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1601/2850 31:19 < 24:28, 0.85 it/s, Epoch 28.07/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.931204</td>\n",
       "      <td>0.616495</td>\n",
       "      <td>0.205498</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.254252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912291</td>\n",
       "      <td>0.616495</td>\n",
       "      <td>0.205498</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.254252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.936900</td>\n",
       "      <td>0.912117</td>\n",
       "      <td>0.616495</td>\n",
       "      <td>0.205498</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.254252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.936900</td>\n",
       "      <td>0.929159</td>\n",
       "      <td>0.616495</td>\n",
       "      <td>0.205498</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.254252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.936700</td>\n",
       "      <td>0.910936</td>\n",
       "      <td>0.616495</td>\n",
       "      <td>0.205498</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.254252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.936700</td>\n",
       "      <td>0.921764</td>\n",
       "      <td>0.616495</td>\n",
       "      <td>0.205498</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.254252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.936700</td>\n",
       "      <td>0.909856</td>\n",
       "      <td>0.616495</td>\n",
       "      <td>0.205498</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.254252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.935400</td>\n",
       "      <td>0.912583</td>\n",
       "      <td>0.616495</td>\n",
       "      <td>0.205498</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.254252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.935400</td>\n",
       "      <td>0.938188</td>\n",
       "      <td>0.583219</td>\n",
       "      <td>0.194406</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.245584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-200\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-200\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-200\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-400\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-400\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-400\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-600\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-600\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-600\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-110] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-800\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-800\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-800\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-120] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-1000\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-1000\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-1000\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-130] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-1200\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-1200\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-1200\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-140] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-1400\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-1400\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-1400\\pytorch_model.bin\n",
      "Deleting older checkpoint [BERT_TUTORIAL_MODEL\\checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 485\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BERT_TUTORIAL_MODEL\\checkpoint-1600\n",
      "Configuration saved in BERT_TUTORIAL_MODEL\\checkpoint-1600\\config.json\n",
      "Model weights saved in BERT_TUTORIAL_MODEL\\checkpoint-1600\\pytorch_model.bin\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\caffe2\\serialize\\inline_container.cc:300] . unexpected pos 40576 vs 40516",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\wsb\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m                 \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wsb\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melement_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m         \u001b[0mzip_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20664/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\wsb\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1438\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1440\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1441\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1442\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wsb\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1569\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1570\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wsb\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[1;34m(self, model, trial, metrics)\u001b[0m\n\u001b[0;32m   1665\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_save\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1666\u001b[0m             \u001b[1;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1667\u001b[1;33m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOPTIMIZER_NAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1668\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcaught_warnings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1669\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSCHEDULER_NAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wsb\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wsb\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\caffe2\\serialize\\inline_container.cc:300] . unexpected pos 40576 vs 40516"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "899ef4cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 727\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='92' max='46' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [46/46 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9347929358482361,\n",
       " 'eval_accuracy': 0.5832187070151307,\n",
       " 'eval_precision': 0.19440623567171023,\n",
       " 'eval_recall': 0.3333333333333333,\n",
       " 'eval_f1': 0.245583550535766,\n",
       " 'eval_runtime': 5.4939,\n",
       " 'eval_samples_per_second': 132.329,\n",
       " 'eval_steps_per_second': 8.373}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79371fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 727\n",
      "  Batch size = 16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "predictions = trainer.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68ae1519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = np.argmax(predictions.predictions,1)\n",
    "sns.heatmap(confusion_matrix(test.label.values,output))#,labels = ['1','-1','0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5168830c-7a43-4de3-837c-8fae2ad2efc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[424,   0,   0],\n",
       "       [214,   0,   0],\n",
       "       [ 89,   0,   0]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test.label.values,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad063e57-d68f-4a52-9221-1fdd253950bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/ProsusAI/finbert/resolve/main/config.json from cache at C:\\Users\\drago/.cache\\huggingface\\transformers\\2120f4f96b5830e5a91fe94d242471b0133b0976c8d6e081594ab837ac5f17bc.ef97278c578016c8bb785f15296476b12eae86423097fed78719d1c8197a3430\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ProsusAI/finbert\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"positive\",\n",
      "    \"1\": \"negative\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 1,\n",
      "    \"neutral\": 2,\n",
      "    \"positive\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/ProsusAI/finbert/resolve/main/vocab.txt from cache at C:\\Users\\drago/.cache\\huggingface\\transformers\\a5b1a5451c9cf1702eec1072ac325d4af10e675a654628eab453b8cba2c6b111.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/ProsusAI/finbert/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/ProsusAI/finbert/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/ProsusAI/finbert/resolve/main/special_tokens_map.json from cache at C:\\Users\\drago/.cache\\huggingface\\transformers\\4c21e8896b03f68c2e028133cf579267c62aba9de03a704a0845704e58eefe9e.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/ProsusAI/finbert/resolve/main/tokenizer_config.json from cache at C:\\Users\\drago/.cache\\huggingface\\transformers\\e3709a60694f45adca209a405cc69ce2b5d47b1cae60696ed9a901426be8c43d.8b6dccc90d16201c6d7ab0f3c6cc38e74b5f2fe587f6efadc9fa71fc0a00c606\n",
      "loading configuration file https://huggingface.co/ProsusAI/finbert/resolve/main/config.json from cache at C:\\Users\\drago/.cache\\huggingface\\transformers\\2120f4f96b5830e5a91fe94d242471b0133b0976c8d6e081594ab837ac5f17bc.ef97278c578016c8bb785f15296476b12eae86423097fed78719d1c8197a3430\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ProsusAI/finbert\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"positive\",\n",
      "    \"1\": \"negative\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 1,\n",
      "    \"neutral\": 2,\n",
      "    \"positive\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/ProsusAI/finbert/resolve/main/config.json from cache at C:\\Users\\drago/.cache\\huggingface\\transformers\\2120f4f96b5830e5a91fe94d242471b0133b0976c8d6e081594ab837ac5f17bc.ef97278c578016c8bb785f15296476b12eae86423097fed78719d1c8197a3430\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ProsusAI/finbert\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"positive\",\n",
      "    \"1\": \"negative\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 1,\n",
      "    \"neutral\": 2,\n",
      "    \"positive\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/ProsusAI/finbert/resolve/main/config.json from cache at C:\\Users\\drago/.cache\\huggingface\\transformers\\2120f4f96b5830e5a91fe94d242471b0133b0976c8d6e081594ab837ac5f17bc.ef97278c578016c8bb785f15296476b12eae86423097fed78719d1c8197a3430\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ProsusAI/finbert\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"positive\",\n",
      "    \"1\": \"negative\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 1,\n",
      "    \"neutral\": 2,\n",
      "    \"positive\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try with finbert \n",
    "#  define tokenizer & model --> this is just a change from the previous code above\n",
    "finbert_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "# turn the configuration for a 3 sentiment classification task\n",
    "config = AutoConfig.from_pretrained(\"ProsusAI/finbert\")\n",
    "config.num_labels = 3\n",
    "\n",
    "finbert_model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "# redefine our datasets as wechanged the tokenizer \n",
    "\n",
    "MAX_LEN = 128\n",
    "train_ds = BERTTutorialDataset(\n",
    "    input_data=train['sentence'].to_numpy(),\n",
    "        sentiment_targets=train['label'].to_numpy(),\n",
    "        tokenizer=finbert_tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "val_ds = BERTTutorialDataset(\n",
    "    input_data=val['sentence'].to_numpy(),\n",
    "        sentiment_targets=val['label'].to_numpy(),\n",
    "        tokenizer=finbert_tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "\n",
    "test_ds = BERTTutorialDataset(\n",
    "    input_data=test['sentence'].to_numpy(),\n",
    "        sentiment_targets=test['label'].to_numpy(),\n",
    "        tokenizer=finbert_tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4368c340-094d-4460-b7a4-3de3c8ae1613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# Define trainning arguments \n",
    "training_args = TrainingArguments('BERT_TUTORIAL_MODEL', overwrite_output_dir=True, evaluation_strategy=\"steps\", \n",
    "                                  num_train_epochs=50, weight_decay=0.005,learning_rate=1e-4,\n",
    "                                  eval_steps=200,metric_for_best_model='accuracy',\n",
    "                                 per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "                                 load_best_model_at_end = True, save_total_limit=10, save_steps=200,no_cuda=False,\n",
    "                             fp16=True,gradient_accumulation_steps=4)\n",
    "trainer = Trainer(\n",
    "    model =finbert_model, args=training_args, train_dataset=train_ds, eval_dataset=val_ds,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=20)], compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bb6407-42b6-465e-b15e-603f7695b308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3633\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 2850\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='2850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  33/2850 00:33 < 51:20, 0.91 it/s, Epoch 0.56/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7195e22d-7a56-4ec3-a1d8-9487a1a55d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88903b98",
   "metadata": {},
   "source": [
    "## 4. Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe53179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers_interpret import SequenceClassificationExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6847a80",
   "metadata": {},
   "source": [
    "To visualise which words in each phrase are the most important for the prediction we will use the python package transformers_interpret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd84b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_model_name = \"ProsusAI/finbert\"\n",
    "model_name = \"textattack/bert-base-uncased-SST-2\"\n",
    "\n",
    "\n",
    "fin_model = AutoModelForSequenceClassification.from_pretrained(fin_model_name)\n",
    "fin_tokenizer = AutoTokenizer.from_pretrained(fin_model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be06563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With both the model and tokenizer initialized we are now able to get explanations on an example text.\n",
    "cls_explainer = SequenceClassificationExplainer(model,\n",
    "                                                tokenizer)\n",
    "\n",
    "fin_cls_explainer = SequenceClassificationExplainer(fin_model,\n",
    "                                                    fin_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9979f669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff99d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_attributions = cls_explainer(\"Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&D and marketing\")\n",
    "word_attributions = fin_cls_explainer(\"Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&D and marketing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf21c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_explainer.predicted_class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6dda98",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_vis = cls_explainer.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050d7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_bert_vis = fin_cls_explainer.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e72ac68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f723887c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
