{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oxford Man Institute NLP Tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blah blah blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Traditional sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and load dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmdict = np.load('data/LoughranMcDonald_dict.npy', allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negation of positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "negate = [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\", \"ain't\", \"aren't\", \"can't\",\n",
    "          \"couldn't\", \"daren't\", \"didn't\", \"doesn't\", \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\",\n",
    "          \"neither\", \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\", \"neednt\", \"needn't\",\n",
    "          \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\", \"oughtnt\", \"shant\", \"shouldnt\", \"wasnt\",\n",
    "          \"werent\", \"oughtn't\", \"shan't\", \"shouldn't\", \"wasn't\", \"weren't\", \"without\", \"wont\", \"wouldnt\", \"won't\",\n",
    "          \"wouldn't\", \"rarely\", \"seldom\", \"despite\", \"no\", \"nobody\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def negated(word):\n",
    "    \"\"\"\n",
    "    Determine if preceding word is a negation word\n",
    "    \"\"\"\n",
    "    if word.lower() in negate:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results with negation check:\n",
      "\n",
      "The # of positive words: 0\n",
      "The # of negative words: 0\n",
      "The list of found positive words: []\n",
      "The list of found negative words: []\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[23, 0, 0, [], []]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tone_count_with_negation_check(dict, article):\n",
    "    \"\"\"\n",
    "    Count positive and negative words with negation check. Account for simple negation only for positive words.\n",
    "    Simple negation is taken to be observations of one of negate words occurring within three words\n",
    "    preceding a positive words.\n",
    "    \"\"\"\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    " \n",
    "    pos_words = []\n",
    "    neg_words = []\n",
    " \n",
    "    input_words = re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', article.lower())\n",
    " \n",
    "    word_count = len(input_words)\n",
    " \n",
    "    for i in range(0, word_count):\n",
    "        if input_words[i] in dict['Negative']:\n",
    "            neg_count += 1\n",
    "            neg_words.append(input_words[i])\n",
    "        if input_words[i] in dict['Positive']:\n",
    "            if i >= 3:\n",
    "                if negated(input_words[i - 1]) or negated(input_words[i - 2]) or negated(input_words[i - 3]):\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i])\n",
    "            elif i == 2:\n",
    "                if negated(input_words[i - 1]) or negated(input_words[i - 2]):\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i])\n",
    "            elif i == 1:\n",
    "                if negated(input_words[i - 1]):\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i])\n",
    "            elif i == 0:\n",
    "                pos_count += 1\n",
    "                pos_words.append(input_words[i])\n",
    " \n",
    "    print('The results with negation check:', end='\\n\\n')\n",
    "    print('The # of positive words:', pos_count)\n",
    "    print('The # of negative words:', neg_count)\n",
    "    print('The list of found positive words:', pos_words)\n",
    "    print('The list of found negative words:', neg_words)\n",
    "    print('\\n', end='')\n",
    " \n",
    "    results = [word_count, pos_count, neg_count, pos_words, neg_words]\n",
    " \n",
    "    return results\n",
    " \n",
    "    \n",
    "# A sample output\n",
    "article = '''Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&D and marketing'''\n",
    " \n",
    "tone_count_with_negation_check(lmdict, article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BERT classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset financial_phrasebank (/Users/danielagorduza/.cache/huggingface/datasets/financial_phrasebank/sentences_50agree/1.0.0/a6d468761d4e0c8ae215c77367e1092bead39deb08fbf4bffd7c0a6991febbf0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb659fdca0048ee92cd5deb407f6629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import all dependencies \n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import  TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from transformers import AutoConfig\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import the dataset from huggingfaces' dataset repository\n",
    "fin_dataset = load_dataset('financial_phrasebank', 'sentences_50agree')\n",
    "df = pd.DataFrame(fin_dataset['train']) # send  it to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is our input sentence : \n",
      " Hi my name is BERT and I am overjoyed  to meet you ! \n",
      "\n",
      "These are the outputs of the tokenizer:\n",
      "\n",
      "{'input_ids': tensor([[  101,  8790,  1139,  1271,  1110,   139,  9637,  1942,  1105,   146,\n",
      "          1821,  1166, 18734,  1174,  1106,  2283,  1128,   106,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "\n",
      "These inputs correspond to the original sentence with separation and padding thrown in :\n",
      "\n",
      "['[CLS] Hi my name is BERT and I am overjoyed to meet you! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n"
     ]
    }
   ],
   "source": [
    "#how does the tokenizer work ? \n",
    "print('\\nThis is our input sentence : \\n Hi my name is BERT and I am overjoyed  to meet you ! \\n')\n",
    "\n",
    "out = tokenizer(['Hi my name is BERT and I am overjoyed  to meet you ! '],\n",
    "          max_length=64,padding=\"max_length\", truncation=True,return_tensors='pt')\n",
    "print('These are the outputs of the tokenizer:\\n')\n",
    "print(out)\n",
    "\n",
    "print('\\nThese inputs correspond to the original sentence with separation and padding thrown in :\\n')\n",
    "print([tokenizer.decode(i) for i in out['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is our model : \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " First layer shape (vocabulary size) : \n",
      "  torch.Size([28996, 768]) \n",
      " Last layer shape (prediction task output shape) : \n",
      "  torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Now that we covered  the tokenizer lets introduce the other building block : the model \n",
    "\n",
    "print('this is our model : \\n')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased')\n",
    "layers = [i for i in model.parameters()]\n",
    "print('\\n First layer shape (vocabulary size) : \\n ',layers[0].shape,\n",
    "'\\n Last layer shape (prediction task output shape) : \\n ',layers[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is our forward  propagation syntax. \n",
      " We feed in a tokenized text and receive the \n",
      " predicted  logits over the 2 classes : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.4133, -0.6355]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic forward propagation of our BERT model \n",
    "print('This is our forward  propagation syntax. \\n We feed in a tokenized text and receive the \\n predicted  logits over the 2 classes : \\n')\n",
    "model.forward(**out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with BERT hands-on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  define tokenizer & model \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# turn the configuration for a 3 sentiment classification task\n",
    "config = AutoConfig.from_pretrained('bert-base-cased')\n",
    "config.num_labels = 3\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_config(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.25, random_state=96)\n",
    "test, val = train_test_split(test, test_size=0.4, random_state=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a Dataset object to put our data in\n",
    "\n",
    "\n",
    "class BERTTutorialDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Special dataset class built on top of the torch Dataset class\n",
    "    useful to have memory efficient dataloading tokenization batching and trainning.\n",
    "    \n",
    "    Huggingface can use these types of dataset as inputs and run all trainning/prediction on them. \n",
    "    \"\"\"\n",
    "    def __init__(self, input_data, sentiment_targets, tokenizer, max_len):\n",
    "        \"\"\"\n",
    "        Basic generator function for the class.\n",
    "        -----------------\n",
    "        input_data : array\n",
    "            Numpy array of string  input text to use for downstream task \n",
    "        sentiment_targets : \n",
    "            Numpy array of integers indexed in  the pytorch style of [0,C-1] with C being the total number of classes\n",
    "            In our example this means the target sentiments should range from 0 to 2. \n",
    "        tokenizer  : Huggingface tokenizer \n",
    "            The huggingface tokenizer to use\n",
    "        max_len : \n",
    "            The truncation length of the tokenizer \n",
    "        -------------------\n",
    "        \n",
    "        Returns : \n",
    "        \n",
    "            Tokenized text with inputs, attentions and labels, ready for the Training script. \n",
    "        \"\"\"\n",
    "        self.input_data = input_data\n",
    "        self.sentiment_targets = sentiment_targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Function required by torch huggingface to batch efficiently\n",
    "        \"\"\"\n",
    "        return len(self.input_data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.input_data[item])\n",
    "        target = self.sentiment_targets[item]\n",
    "        # only difference with the previuous tokenization step is the encode-plus for special tokens\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          text,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          return_token_type_ids=False,\n",
    "          padding='max_length',\n",
    "          return_attention_mask=True,\n",
    "          return_tensors='pt',\n",
    "          truncation = True\n",
    "        )\n",
    "        return {\n",
    "          'text': text,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'labels': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our train-val-test datasets\n",
    "MAX_LEN = 32\n",
    "train_ds = BERTTutorialDataset(\n",
    "    input_data=train['sentence'].to_numpy(),\n",
    "        sentiment_targets=train['label'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "val_ds = BERTTutorialDataset(\n",
    "    input_data=val['sentence'].to_numpy(),\n",
    "        sentiment_targets=val['label'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "\n",
    "test_ds = BERTTutorialDataset(\n",
    "    input_data=test['sentence'].to_numpy(),\n",
    "        sentiment_targets=test['label'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some accuracy measure ( helpful for the early stopping )\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    Function to calculate accuracies and losses for the validation from the predicted outputs\n",
    "    This is neccessary for the early stopping. \n",
    "    \"\"\"\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred, average='macro')\n",
    "    precision = precision_score(y_true=labels, y_pred=pred, average='macro')\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred, average='macro')    \n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define trainning arguments \n",
    "training_args = TrainingArguments('BERT_TUTORIAL_MODEL', overwrite_output_dir=True, evaluation_strategy=\"steps\", \n",
    "                                  num_train_epochs=3, weight_decay=0.005,learning_rate=1e-4,\n",
    "                                  eval_steps=10,metric_for_best_model='accuracy',\n",
    "                                 per_device_train_batch_size=128, per_device_eval_batch_size=128,\n",
    "                                 load_best_model_at_end = True, save_total_limit=2, save_steps=10,no_cuda=True\n",
    "                             )\n",
    "trainer = Trainer(\n",
    "    model =model, args=training_args, train_dataset=train_ds, eval_dataset=val_ds,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=20)], compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='12' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/45 10:33 < 34:48, 0.02 it/s, Epoch 0.73/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.887540</td>\n",
       "      <td>0.637113</td>\n",
       "      <td>0.212371</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.259446</td>\n",
       "      <td>25.484200</td>\n",
       "      <td>19.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912950</td>\n",
       "      <td>0.595598</td>\n",
       "      <td>0.198533</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.248851</td>\n",
       "      <td>37.894300</td>\n",
       "      <td>19.185000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9129496216773987,\n",
       " 'eval_accuracy': 0.5955983493810179,\n",
       " 'eval_precision': 0.19853278312700595,\n",
       " 'eval_recall': 0.3333333333333333,\n",
       " 'eval_f1': 0.24885057471264369,\n",
       " 'eval_runtime': 37.8943,\n",
       " 'eval_samples_per_second': 19.185}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "predictions = trainer.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.595598\n",
       "2    0.299862\n",
       "0    0.104539\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.label.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5da6c6da38d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#,labels = ['1','-1','0']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = np.argmax(predictions.predictions,1)\n",
    "sns.heatmap(confusion_matrix(test.label.values,output))#,labels = ['1','-1','0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers_interpret import SequenceClassificationExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise which words in each phrase are the most important for the prediction we will use the python package transformers_interpret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_model_name = \"ProsusAI/finbert\"\n",
    "model_name = \"textattack/bert-base-uncased-SST-2\"\n",
    "\n",
    "\n",
    "fin_model = AutoModelForSequenceClassification.from_pretrained(fin_model_name)\n",
    "fin_tokenizer = AutoTokenizer.from_pretrained(fin_model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With both the model and tokenizer initialized we are now able to get explanations on an example text.\n",
    "cls_explainer = SequenceClassificationExplainer(model,\n",
    "                                                tokenizer)\n",
    "\n",
    "fin_cls_explainer = SequenceClassificationExplainer(fin_model,\n",
    "                                                    fin_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_attributions = cls_explainer(\"Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&D and marketing\")\n",
    "word_attributions = fin_cls_explainer(\"Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&D and marketing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_explainer.predicted_class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_vis = cls_explainer.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_bert_vis = fin_cls_explainer.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
