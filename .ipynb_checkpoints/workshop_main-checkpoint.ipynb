{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ae4cd8",
   "metadata": {},
   "source": [
    "# Oxford Man Institute NLP Tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d7c68",
   "metadata": {},
   "source": [
    "## 1. Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e5b5a0",
   "metadata": {},
   "source": [
    "There are several ways of performing sentiment classification on a document or article, ranging from word-counts to modern Transformer-based Language Models. In this tutorial we will take you through a range of classification techniques:\n",
    "- Loughran & McDonald financial sentiment dictionary\n",
    "- Naive Bayes Classifier\n",
    "- BERT out of the box\n",
    "- BERT fine-tuned on general sentiment datasets\n",
    "- FinBERT \n",
    "    - BERT that has been trained on positive and negative financial documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4029a21a-8149-4aa9-b726-2064e745b9ef",
   "metadata": {},
   "source": [
    "## 2. Load and analyse the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272fa01e",
   "metadata": {},
   "source": [
    "### Import packages and load dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8c3d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import inspect\n",
    "import csv\n",
    "import operator\n",
    "from omi_nlp_tools import mutual_information # function to get mutual information \n",
    "# import my_functions\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c00663a-8d6a-4ea3-8b10-680d917b5183",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10f05db8-fca7-4974-ac3f-bb685a89fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(r'data/kaggle_data.csv')\n",
    "# df.columns = ['text', 'class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d17b5ac4-3480-44c0-9083-4b9e4780e34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drago\\AppData\\Local\\Temp\\ipykernel_23704\\3544662773.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv('data\\\\FinancialPhraseBank-v1.0\\\\Sentences_AllAgree.txt',\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In the third quarter of 2010 , net sales incre...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Operating profit rose to EUR 13.1 mn from EUR ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Operating profit totalled EUR 21.1 mn , up fro...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     class\n",
       "0  According to Gran , the company has no plans t...   neutral\n",
       "1  For the last quarter of 2010 , Componenta 's n...  positive\n",
       "2  In the third quarter of 2010 , net sales incre...  positive\n",
       "3  Operating profit rose to EUR 13.1 mn from EUR ...  positive\n",
       "4  Operating profit totalled EUR 21.1 mn , up fro...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data\\\\FinancialPhraseBank-v1.0\\\\Sentences_AllAgree.txt',\n",
    "            encoding = 'ISO-8859-1',on_bad_lines='skip',sep = '.@', names=['text', 'class'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89c750f-eee1-40a9-b6af-812ee7a1411b",
   "metadata": {},
   "source": [
    "#### What kind of text are we dealing with?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4656f863-165f-4af2-8d23-bbcc47ff1ca5",
   "metadata": {},
   "source": [
    "The Financial PhraseBank was used by Maks and Vossen (2014) in their paper ***\"Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts\"***. The authors scraped the LexisNexis database for news articles about companies on the OMX Helsinki database.\n",
    "\n",
    "Although very small, the dataset has since been widely used in the financial community to test and deploy domain specific language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fa191de-e03f-48f7-a236-dcbd19b8ed94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest positive sentence is: \n",
      "\n",
      " 831    MANAVIGATOR-September 7 , 2010-Kemira unveils Indian JV with IVRCL Finnish chemicals group Kemira ( HEL : KRA1V ) on Tuesday announced it has inked a deal to form a joint venture in India with local construction firm IVRCL Infrastructure and Projects Ltd ( BOM :530773 ) \n",
      "Name: text, dtype: object \n",
      "\n",
      "\n",
      "The longest negative sentence is: \n",
      "\n",
      " 2116    Finnish Exel Composites , a technology company that designs , manufactures , and markets composite profiles and tubes for various industrial applications , reports its net sales decreased by 0.6 % in the second quarter of 2010 to EUR 19.2 mn from EUR 19.3 mn in the corresponding period in 2009 \n",
      "Name: text, dtype: object \n",
      "\n",
      "\n",
      "The longest neutral sentence is: \n",
      "\n",
      " 764    Supported Nokia phones include : N96 , N95-8GB , N95 , N93-N931 , N92 , N85 , N82 , N81 , N80 , N79 , N78 , N77 , N76 , N75 , N73 , N72 , N71 , E90 , E71 , E70 , E66 , E65 , E62 , E61-E61i , E60 , E51 , E50 , Touch Xpress 5800 , 6220 Classic , 6210 Navigator , 6120 Classic , 6110 Navigator , 5700 , 5500 , 5320XM \n",
      "Name: text, dtype: object \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cata in ['positive','negative','neutral']:\n",
    "    cata_df = df[df['class'] == cata]\n",
    "    cata_lens = cata_df['text'].str.len()\n",
    "    max_sentence = cata_df[cata_lens == cata_lens.max()]\n",
    "    with pd.option_context('display.max_colwidth', 500):\n",
    "        print(f'The longest {cata} sentence is: \\n\\n', max_sentence['text'], '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac12bc77-18c8-400c-9ff8-cefcc2148b19",
   "metadata": {},
   "source": [
    "#### Data formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26553ef6-a6e1-4dec-ba10-604704c86e41",
   "metadata": {},
   "source": [
    "The dataset we are using has already been cleaned as it is widely used in the field. If you are using your own custom dataset it may be necessary to clean the text of special characters, URLs, user mentions, emojis etc. \n",
    "\n",
    "We will add another columns to our dataset that contains a **tokenised version** of the text as this will form the basis of our Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e593fd5-367f-4b2b-8f88-b63858090acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "df[\"tokenised_text\"] = df.text.apply(lambda x: token_pattern.findall(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54c1c79-7c2e-46c0-8e0b-88a30da01aef",
   "metadata": {},
   "source": [
    "**Remove the neutral category** so that we are left with a more extreme examples in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2632802-7f69-4b93-ab2f-d412eaaa7a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['class'] != 'neutral']\n",
    "categories = list(set(df['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "448becfe-ea9c-475a-b61d-3e5c19b4c97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "873 entries in the dataset\n",
      "303 entries / 34.71% labelled as negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_negative = df[df['class']==\"negative\"].shape[0]\n",
    "prop_negative = df[df['class']==\"negative\"].shape[0]/df.shape[0]\n",
    "\n",
    "print(f\"{df.shape[0]} entries in the dataset\")\n",
    "print(\"{} entries / {:.2%} labelled as negative\\n\".format(n_negative, prop_negative)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a333bc-6838-4845-ab02-85018db8497a",
   "metadata": {},
   "source": [
    "#### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee3dd591-1144-4487-aa06-44255170abc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, dev_text, test_text = dict(), dict(), dict()\n",
    "\n",
    "for c_i in categories:\n",
    "    train_text[c_i], devtest_text = train_test_split(df[df['class']==c_i].tokenised_text,\n",
    "                                                         train_size=0.7, random_state=123)\n",
    "    dev_text[c_i], test_text[c_i] = train_test_split(devtest_text,\n",
    "                                                         train_size=0.5, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a76f108-3939-4549-946b-d5bc2bc5ba9f",
   "metadata": {},
   "source": [
    "Create vocab dict and remove stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d1ecb88-57f6-412d-9fe9-9308b163826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocab dict\n",
    "vocab = defaultdict(Counter)\n",
    "n_text = defaultdict(Counter)\n",
    "\n",
    "# Create vocabularies\n",
    "for c_i in categories:\n",
    "    for text in train_text[c_i]:\n",
    "        vocab[c_i].update(text)\n",
    "        n_text[c_i].update(set(text))\n",
    "\n",
    "# Remove stopwords\n",
    "stopwords = list()\n",
    "\n",
    "with open('./data/stopwords.txt', 'r') as fd:\n",
    "    reader = csv.reader(fd)\n",
    "    for row in fd:\n",
    "        stopwords.append(row.replace(\"\\n\",\"\"))\n",
    "        \n",
    "for c_i in categories:\n",
    "    for sw in stopwords:\n",
    "        del vocab[c_i][sw]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d11b66-cc21-4567-a8be-09730acb92b1",
   "metadata": {},
   "source": [
    "#### Inspection of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c6e436c-3452-44f7-be91-79bc1deb8eea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in POSITIVE class:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('EUR', 237),\n",
       " ('mn', 157),\n",
       " ('profit', 114),\n",
       " ('year', 106),\n",
       " ('net', 103),\n",
       " ('period', 88),\n",
       " ('million', 82),\n",
       " ('sales', 80),\n",
       " ('Finnish', 75),\n",
       " ('mln', 71)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most common words in NEGATIVE class:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('EUR', 174),\n",
       " ('mn', 125),\n",
       " ('profit', 83),\n",
       " ('year', 57),\n",
       " ('net', 52),\n",
       " ('quarter', 51),\n",
       " ('mln', 50),\n",
       " ('sales', 50),\n",
       " ('2008', 44),\n",
       " ('million', 43)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for c_i in categories:\n",
    "    print(f\"Most common words in {c_i.upper()} class:\")\n",
    "    display(vocab[c_i].most_common(10))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb892344-e0b9-4e98-930a-f6e32dbdc065",
   "metadata": {},
   "source": [
    "#### Mututal information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8a374c-4730-4ca6-9a5e-5ad9bb94132f",
   "metadata": {},
   "source": [
    "Mutual information can help us explain the differences between word distributions. Understanding the features that differentiate a certain category from another can prove very useful for interpretting differences between categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4d80aac-6ce1-4b2f-b2da-374ea13876f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of articles in each class, which we will need to calculate mutual information\n",
    "n_total = dict()\n",
    "for c_i in categories:\n",
    "    n_total[c_i] = len(train_text[c_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00532125-9f5e-4d78-b818-ccec005d1622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 overall most informative features:\n",
      "\n",
      " ['fell', 'increased', 'half', '2008', 'nine', 'due', 'EUR', '17', '2007', 'mn', 'quarter', 'eur', 'profit', 'Helsinki', 'first', 'trade', 'prices', 'electronics', '2010', 'excluding', 'items', 'reports', 'negative', 'third', 'order']\n"
     ]
    }
   ],
   "source": [
    "mi_list = sorted([(mutual_information(w, n_text, n_total), w) for w in set(vocab['negative']).union(set(vocab['positive']))], reverse=True)\n",
    "\n",
    "print('25 overall most informative features:\\n\\n', [w for mi, w in mi_list][:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0879d0bc",
   "metadata": {},
   "source": [
    "## 3. Traditional sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d35271",
   "metadata": {},
   "source": [
    "### Loughran & McDonald classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5446979d-60e4-4d7a-9597-d608a36170a0",
   "metadata": {},
   "source": [
    "Loughran & McDonald released their master dictionary in 2011 in conjunction with their paper “When is a Liability not a Liability? Textual Analysis, Dictionaries, and 10-Ks\". The dictionary lists a number of words and includes negative, positive, uncertainty, litigious, strong modal, weak modal, and constraining tags. \n",
    "\n",
    "There are several shortcomings to this simplistic approach:\n",
    "\n",
    "- **Some words don't appear in the dictionary (fall, rise, etc.)**\n",
    "- **Some words are negative/positive given the context they are written (profit, expenditure, etc.)**\n",
    "- **Simple counts of words don't necessarily infer the overall sentiment**\n",
    "    - *Hatred for football has always confused me; there are so many haters who attack the sport, but I have always loved it.* - 3 negative words and 1 positive word.\n",
    "\n",
    "\n",
    "We have taken the words that have a negative and positive tag for our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "608eae79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some examples of negative words:  ['abandon', 'abandoned', 'abandoning', 'abandonment', 'abandonments']\n",
      "Some examples of positive words:  ['able', 'abundance', 'abundant', 'acclaimed', 'accomplish']\n"
     ]
    }
   ],
   "source": [
    "lmdict = np.load('data/LoughranMcDonald_dict.npy', allow_pickle='TRUE').item()\n",
    "print('Some examples of negative words: ', lmdict['Negative'][:5])\n",
    "print('Some examples of positive words: ', lmdict['Positive'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "441f95e2-be4a-4263-96da-9dc07d68a45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able', 'abundance', 'abundant', 'acclaimed', 'accomplish']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmdict['Positive'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00495202-583a-4b11-8ede-7cfb61fd41db",
   "metadata": {},
   "source": [
    "Check to see if a word appears in the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "004cd19d-8fbe-4e5c-9538-2cfc0017d73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, fall is not in the Loughran & McDonald dictionary\n"
     ]
    }
   ],
   "source": [
    "word = 'fall'\n",
    "\n",
    "if word in lmdict['Negative']:\n",
    "    print(f'Yes, {word} is a Negative word in the Loughran & McDonald dictionary')\n",
    "elif word in lmdict['Positive']:\n",
    "    print(f'Yes, {word} is Positive word in the Loughran & McDonald dictionary')\n",
    "else:\n",
    "    print(f'No, {word} is not in the Loughran & McDonald dictionary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f1465",
   "metadata": {},
   "source": [
    "Negation is another challenge that emerges using this approach. A techy fix is to check if the word is preceeded by a negating word in our list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48d384c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "negate = [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\", \"ain't\", \"aren't\", \"can't\",\n",
    "          \"couldn't\", \"daren't\", \"didn't\", \"doesn't\", \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\",\n",
    "          \"neither\", \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\", \"neednt\", \"needn't\",\n",
    "          \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\", \"oughtnt\", \"shant\", \"shouldnt\", \"wasnt\",\n",
    "          \"werent\", \"oughtn't\", \"shan't\", \"shouldn't\", \"wasn't\", \"weren't\", \"without\", \"wont\", \"wouldnt\", \"won't\",\n",
    "          \"wouldn't\", \"rarely\", \"seldom\", \"despite\", \"no\", \"nobody\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a0377ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def negated(word):\n",
    "    \"\"\"\n",
    "    Determine if preceding word is a negation word\n",
    "    \"\"\"\n",
    "    if word.lower() in negate:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcd91aa",
   "metadata": {},
   "source": [
    "This function counts the number of negative and positive words in a document and performs a negation check to switch the polarity of words that are preceeded by a word in the *negate* list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ec407be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results with negation check:\n",
      "\n",
      "The # of positive words: 0\n",
      "The # of negative words: 6\n",
      "The list of found positive words: []\n",
      "The list of found negative words: ['worst', 'worse', 'recession', 'fears', 'recessions', 'decline']\n",
      "The overall classification is: negative\n"
     ]
    }
   ],
   "source": [
    "def tone_count_with_negation_check(dict, article):\n",
    "    \"\"\"\n",
    "    Count positive and negative words with negation check. Account for simple negation only for positive words.\n",
    "    Simple negation is taken to be observations of one of negate words occurring within three words\n",
    "    preceding a positive words.\n",
    "    \"\"\"\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    " \n",
    "    pos_words = []\n",
    "    neg_words = []\n",
    " \n",
    "    input_words = re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', article.lower())\n",
    " \n",
    "    word_count = len(input_words)\n",
    " \n",
    "    for i in range(0, word_count):\n",
    "        if input_words[i] in dict['Negative']:\n",
    "            neg_count += 1\n",
    "            neg_words.append(input_words[i])\n",
    "        if input_words[i] in dict['Positive']:\n",
    "            if i >= 3:\n",
    "                if negated(input_words[i - 1]) or negated(input_words[i - 2]) or negated(input_words[i - 3]):\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i])\n",
    "            elif i == 2:\n",
    "                if negated(input_words[i - 1]) or negated(input_words[i - 2]):\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i])\n",
    "            elif i == 1:\n",
    "                if negated(input_words[i - 1]):\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i])\n",
    "            elif i == 0:\n",
    "                pos_count += 1\n",
    "                pos_words.append(input_words[i])\n",
    "                \n",
    "    sentiment_score = len(pos_words) - len(neg_words)\n",
    "    \n",
    "    if sentiment_score < 0:\n",
    "        classification = 'negative'\n",
    "    elif sentiment_score > 0:\n",
    "        classification = 'positive'\n",
    "    else:\n",
    "        classification = 'neutral'\n",
    " \n",
    "    results = [word_count, pos_count, neg_count, pos_words, neg_words, classification]\n",
    " \n",
    "    return results\n",
    " \n",
    "    \n",
    "# A sample output\n",
    "article = '''The stock market has had its worst start to a year in recent history and things could get \n",
    "             worse as recession fears loom. Since World War II there have been 13 recessions—defined as \n",
    "             two consecutive quarters of GDP decline–and there have been 3 in the 21st century (2001, 2008 and 2020), \n",
    "             according to the National Bureau of Economic Research. Some experts say another one could be on the way.'''\n",
    " \n",
    "sent_results = tone_count_with_negation_check(lmdict, article)\n",
    "\n",
    "print('The results with negation check:', end='\\n\\n')\n",
    "print('The # of positive words:', sent_results[1])\n",
    "print('The # of negative words:', sent_results[2])\n",
    "print('The list of found positive words:', sent_results[3])\n",
    "print('The list of found negative words:', sent_results[4])\n",
    "print('The overall classification is:', sent_results[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dee1da-6f30-491a-b31a-eb0e877862f7",
   "metadata": {},
   "source": [
    "Sometimes this technique can be successful, but there are many examples where it falls short. For example, when we use a clearly negative text that doesn't contain any words in the lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d2cdaf7-6459-4ad0-92a9-00f377034870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results with negation check:\n",
      "\n",
      "The # of positive words: 0\n",
      "The # of negative words: 0\n",
      "The list of found positive words: []\n",
      "The list of found negative words: []\n",
      "The overall classification is: neutral\n"
     ]
    }
   ],
   "source": [
    "article = '''Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that \n",
    "             were hit by larger expenditures on R&D and marketing'''\n",
    " \n",
    "sent_results = tone_count_with_negation_check(lmdict, article)\n",
    "\n",
    "print('The results with negation check:', end='\\n\\n')\n",
    "print('The # of positive words:', sent_results[1])\n",
    "print('The # of negative words:', sent_results[2])\n",
    "print('The list of found positive words:', sent_results[3])\n",
    "print('The list of found negative words:', sent_results[4])\n",
    "print('The overall classification is:', sent_results[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1627da33-0944-4089-88aa-c499aecb0b7f",
   "metadata": {},
   "source": [
    "#### Performance on the Financial PhraseBank dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15ff9a6c-393a-4d33-961f-8fcfeb9eaa49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = list(df['class'])\n",
    "predictions= list()\n",
    "\n",
    "for phrase, c_i in zip(df['text'], df['class']):\n",
    "    sent_results = tone_count_with_negation_check(lmdict, phrase)\n",
    "    classification = sent_results[5]\n",
    "    predictions.append(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "711dba89-2fea-4410-82ed-93db1b99740d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We passed 873 examples through the LM classifier \n",
      "\n",
      "There were 573 NEUTRAL predictions\n",
      "There were 101 POSITIVE predictions\n",
      "There were 199 NEGATIVE predictions\n"
     ]
    }
   ],
   "source": [
    "neutral_count = predictions.count('neutral')\n",
    "positive_count = predictions.count('positive')\n",
    "negative_count = predictions.count('negative')\n",
    "\n",
    "print(f'We passed {len(predictions)} examples through the LM classifier \\n')\n",
    "print(f'There were {neutral_count} NEUTRAL predictions')\n",
    "print(f'There were {positive_count} POSITIVE predictions')\n",
    "print(f'There were {negative_count} NEGATIVE predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ecba44f5-cbe7-4ab8-b473-609628f1f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.613     0.403     0.486       303\n",
      "     neutral      0.000     0.000     0.000         0\n",
      "    positive      0.921     0.163     0.277       570\n",
      "\n",
      "    accuracy                          0.246       873\n",
      "   macro avg      0.511     0.189     0.254       873\n",
      "weighted avg      0.814     0.246     0.350       873\n",
      "\n",
      "f1 score :  0.2544179961722905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drago\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\drago\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\drago\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels, predictions, digits=3))\n",
    "print('f1 score : ',f1_score(labels, predictions, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3033173-1b0d-42b9-9a94-5e46ded9b4ef",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9589ae9f-842e-4b6e-89b7-2bd147d759f0",
   "metadata": {},
   "source": [
    "A NB classifier improves upon the lexicon technique explored above as it utilises the individual contribution of each word in the text. The fundamental training goal of a NB classifier is to calculate the individual probability of a word appearing in a particular class, $P(w|c_i)$, and summing the log probability of each word to discern $P(d|c_i)$. \n",
    "\n",
    "There are some assumptions and training details in our model:\n",
    "- Add-one smoothing - assume that all words appear once in each class so that one can calculate the probability for a word appearing.\n",
    "    - Large oversimplification but it is necessary for both classes to have the same support.\n",
    "- Remove all stop words - probability distribution of stop words doesn't neccessarily discern sentiment classification.\n",
    "    - she, he, it, so, I, etc. \n",
    "    \n",
    "##### Advantages and disadvatanges:\n",
    "    \n",
    "<font color='green'>$\\checkmark$</font>   Granular feature set \\\n",
    "<font color='green'>$\\checkmark$</font>   Can also include bigrams, trigrams, emojis etc. for improved performance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77298ce7-dc82-42a1-9103-240a38a4e9cb",
   "metadata": {},
   "source": [
    "#### Training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1a95616-a1c6-407b-85c2-c82cec38e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes with additive smoothing\n",
    "# Define function to get P(w|c_i), class-conditional propbabilities for w\n",
    "\n",
    "def naive_bayes_additive_smoothing(vocab, categories, smoothing_alpha):\n",
    "    \n",
    "    # Calculate unsmoothed probabilities\n",
    "    probabilities = dict()\n",
    "    \n",
    "    for c_i in categories:\n",
    "        \n",
    "        probabilities[c_i] = dict()\n",
    "        \n",
    "        # First, consider all words that are in the vocab for either class\n",
    "        for word in set(vocab[\"hateful\"]).union(set(vocab[\"non-hateful\"])):\n",
    "            # If they do exist in the current class c_i, store their count --> 1st order model\n",
    "            if vocab[c_i][word]>0:\n",
    "                probabilities[c_i][word] = vocab[c_i][word]\n",
    "            # If not, set their count to be the smoothing parameter (rather than excluding them, as we did for no smoothing) --> backoff to 0th order model\n",
    "            else:\n",
    "                probabilities[c_i][word] = smoothing_alpha\n",
    "        \n",
    "        # Second, we take the sum of counts of words in this new dict\n",
    "        total = sum(probabilities[c_i].values())\n",
    "        \n",
    "        # Last, we turn the counts for each word into probabilities by dividing them by that sum\n",
    "        probabilities[c_i] = {word: probabilities[c_i][word] / total for word in probabilities[c_i]}\n",
    "    \n",
    "    return probabilities\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84ef887-ee24-4a55-b2c2-379affc1f327",
   "metadata": {},
   "source": [
    "Estimate the probability of the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9d0695f-2de6-465a-8bf6-ca7e95ce8dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate P(c_i), the probability of class c_i, based on the class distribution in the test set\n",
    "prob_class = dict()\n",
    "for c_i in categories:\n",
    "    prob_class[c_i] = train_text[c_i].shape[0]/(train_text[\"negative\"].shape[0]+train_text[\"positive\"].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4848084-5a79-4d05-afff-827da94b4f18",
   "metadata": {},
   "source": [
    "Retrieve the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "993c87f6-c92a-43b2-9b4d-ecd94c775cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_predictions(categories, test_tweets, probabilities, prob_class):\n",
    "\n",
    "    # Initialize lists for storing ground truth labels and predictions\n",
    "    labels = list()\n",
    "    predictions = list()\n",
    "\n",
    "    # Loop over categories\n",
    "    for c_i in categories:\n",
    "\n",
    "        # Loop over test tweets\n",
    "        for tweet in test_tweets[c_i]:\n",
    "\n",
    "            # Store ground truth\n",
    "            labels.append(c_i)\n",
    "\n",
    "            # For each post, calculate scores for each of the two categories\n",
    "            scores = {'negative': 0, 'positive': 0}\n",
    "            for word in tweet:\n",
    "                if word in probabilities[c_i]:\n",
    "                    scores[\"negative\"] += np.log(probabilities[\"negative\"][word])\n",
    "                    scores[\"positive\"] += np.log(probabilities[\"positive\"][word])\n",
    "\n",
    "            # Class imbalance\n",
    "            scores[\"negative\"] = scores[\"negative\"]+np.log(prob_class[\"negative\"])\n",
    "            scores[\"positive\"] = scores[\"positive\"]+np.log(prob_class[\"positive\"])\n",
    "\n",
    "            # Use higher score for prediction\n",
    "            predictions.append(max(scores.items(), key=operator.itemgetter(1))[0])\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65c59178-f511-4404-9c29-1e9e1dbda016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.000     0.000     0.000        45\n",
      "    positive      0.654     1.000     0.791        85\n",
      "\n",
      "    accuracy                          0.654       130\n",
      "   macro avg      0.327     0.500     0.395       130\n",
      "weighted avg      0.428     0.654     0.517       130\n",
      "\n",
      "f1 score 0.39534883720930236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drago\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\drago\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\drago\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "probs = naive_bayes_additive_smoothing(vocab, categories, smoothing_alpha=0.1)\n",
    "\n",
    "# Get predictions on dev set\n",
    "labels, predictions = get_nb_predictions(categories, dev_text, probs, prob_class)\n",
    "\n",
    "# Calculate and store macro F1 on test set\n",
    "print(classification_report(labels, predictions, digits=3))\n",
    "print('f1 score : ',f1_score(labels, predictions, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12738db-5639-45d5-b6b7-124c6b7a1553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb7855-f01a-4b38-9c5e-f9f9ce7d9a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b7a1719-18c7-46fe-8487-37cbf180b3ab",
   "metadata": {},
   "source": [
    "## 3. BERT classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03516dc1-abee-498f-a856-804b47983c23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import all dependencies \n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer,logging\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer,BertForMaskedLM,AutoTokenizer\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import  TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from transformers import AutoConfig\n",
    "import warnings\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd00440-17d2-4f02-bc0a-5e07b37cffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91227355-8b5c-4bb6-bcc8-2485bdb4949c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the dataset from huggingfaces' dataset repository\n",
    "#fin_dataset = load_dataset('financial_phrasebank', 'sentences_allagree')\n",
    "#df = pd.DataFrame(fin_dataset['train']) # send  it to a pandas dataframe\n",
    "\n",
    "# If you are having issues running the code above on Windows, don't worry this is a known Huggingface error-\n",
    "# Please use the code below which wille import the data we were planning to use from a .txt file\n",
    "\n",
    "#origin of this data : data/FinancialPhraseBanl-v1.0\n",
    "\n",
    "df = pd.read_csv('data\\\\FinancialPhraseBank-v1.0\\\\Sentences_50Agree.txt',\n",
    "            encoding = 'ISO-8859-1',on_bad_lines='skip',sep = '.@')\n",
    "df.columns = ['sentence','label']\n",
    "df['label'] = df['label'].replace(to_replace=({'neutral':2,'positive':0,'negative':1}))\n",
    "df['label']= df['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b653abe5-5242-4ff3-b0c7-0595e6f4b7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make the task comparable to the Bayesian classifier above we will also drop the neutral class \n",
    "\n",
    "df = df[df['label']!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a2858",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395ff2f9-a731-40c1-870a-55b23cbfb18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22345d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e1335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how does the tokenizer work ? \n",
    "print('\\nThis is our input sentence : \\n Hi my name is BERT and I am overjoyed  to meet you ! \\n')\n",
    "\n",
    "out = tokenizer(['Hi my name is BERT and I am overjoyed  to meet you ! '],\n",
    "          max_length=64,padding=\"max_length\", truncation=True,return_tensors='pt')\n",
    "print('These are the outputs of the tokenizer:\\n')\n",
    "print(out)\n",
    "\n",
    "print('\\nThese inputs correspond to the original sentence with separation and padding thrown in :\\n')\n",
    "print([tokenizer.decode(i) for i in out['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed749a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we covered  the tokenizer lets introduce the other building block : the model \n",
    "\n",
    "print('this is our model : \\n')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased')\n",
    "layers = [i for i in model.parameters()]\n",
    "print('\\n First layer shape (vocabulary size) : \\n ',layers[0].shape,\n",
    "      '\\n Number of self attention heads: ',len([i.shape  for i in layers if (i.shape==torch.Size([3072])) ]),\n",
    "'\\n Last layer shape (prediction task output shape) : \\n ',layers[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c745e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic forward propagation of our BERT model \n",
    "print('This is our forward propagation syntax. \\n We feed in a tokenized text and receive the \\n predicted  logits over the 2 classes : \\n')\n",
    "model_output = model.forward(**out)\n",
    "print(model_output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebec1e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with BERT hands-on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf53dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  define tokenizer & model \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# turn the configuration for a 2 sentiment classification task\n",
    "config = AutoConfig.from_pretrained('bert-base-cased')\n",
    "config.num_labels = 2\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_config(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ed0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.25, random_state=96)\n",
    "test, val = train_test_split(test, test_size=0.4, random_state=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38988d-5ed7-4eaa-9c28-6c6ddef1ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.label.value_counts(normalize=True),'\\n',\n",
    "test.label.value_counts(normalize=True),'\\n',\n",
    "val.label.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3399a3e-ba2d-4e27-bffd-1bc533f63b45",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining a dataset class to interact with the Huggingface Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f2be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a Dataset object to put our data in\n",
    "\n",
    "\n",
    "class BERTTutorialDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Special dataset class built on top of the torch Dataset class\n",
    "    useful to have memory efficient dataloading tokenization batching and trainning.\n",
    "    \n",
    "    Huggingface can use these types of dataset as inputs and run all trainning/prediction on them. \n",
    "    \"\"\"\n",
    "    def __init__(self, input_data, sentiment_targets, tokenizer, max_len):\n",
    "        \"\"\"\n",
    "        Basic generator function for the class.\n",
    "        -----------------\n",
    "        input_data : array\n",
    "            Numpy array of string  input text to use for downstream task \n",
    "        sentiment_targets : \n",
    "            Numpy array of integers indexed in  the pytorch style of [0,C-1] with C being the total number of classes\n",
    "            In our example this means the target sentiments should range from 0 to 2. \n",
    "        tokenizer  : Huggingface tokenizer \n",
    "            The huggingface tokenizer to use\n",
    "        max_len : \n",
    "            The truncation length of the tokenizer \n",
    "        -------------------\n",
    "        \n",
    "        Returns : \n",
    "        \n",
    "            Tokenized text with inputs, attentions and labels, ready for the Training script. \n",
    "        \"\"\"\n",
    "        self.input_data = input_data\n",
    "        self.sentiment_targets = sentiment_targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Function required by torch huggingface to batch efficiently\n",
    "        \"\"\"\n",
    "        return len(self.input_data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.input_data[item])\n",
    "        target = self.sentiment_targets[item]\n",
    "        # only difference with the previuous tokenization step is the encode-plus for special tokens\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          text,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          return_token_type_ids=False,\n",
    "          padding='max_length',\n",
    "          return_attention_mask=True,\n",
    "          return_tensors='pt',\n",
    "          truncation = True\n",
    "        )\n",
    "        return {\n",
    "          'text': text,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'labels': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a52df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our train-val-test datasets\n",
    "MAX_LEN = 128\n",
    "train_ds = BERTTutorialDataset(\n",
    "    input_data=train['sentence'].to_numpy(),\n",
    "        sentiment_targets=train['label'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "val_ds = BERTTutorialDataset(\n",
    "    input_data=val['sentence'].to_numpy(),\n",
    "        sentiment_targets=val['label'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "\n",
    "test_ds = BERTTutorialDataset(\n",
    "    input_data=test['sentence'].to_numpy(),\n",
    "        sentiment_targets=test['label'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc2a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some accuracy measure ( helpful for the early stopping )\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import numpy as np\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    Function to calculate accuracies and losses for the validation from the predicted outputs\n",
    "    This is neccessary for the early stopping. \n",
    "    \"\"\"\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred, average='macro')\n",
    "    precision = precision_score(y_true=labels, y_pred=pred, average='macro')\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred, average='macro')    \n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cb9410-4e7e-4535-ba5d-da3dc009bfa2",
   "metadata": {},
   "source": [
    "## Defining the trainning arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b300ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define trainning arguments \n",
    "training_args = TrainingArguments('BERT_TUTORIAL_MODEL', overwrite_output_dir=True, evaluation_strategy=\"steps\", \n",
    "                                  num_train_epochs=50, weight_decay=1e-8,learning_rate=1e-5,\n",
    "                                  eval_steps=50,metric_for_best_model='accuracy',\n",
    "                                 per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "                                 load_best_model_at_end = True, save_total_limit=10, save_steps=50,no_cuda=False,\n",
    "                             fp16=True,gradient_accumulation_steps=4)\n",
    "trainer = Trainer(\n",
    "    model =model, args=training_args, train_dataset=train_ds, eval_dataset=val_ds,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=5)], compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef15230a-2b50-45fe-836b-bc216dda9cd1",
   "metadata": {},
   "source": [
    "### Lauching the training preocess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf13f2e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ef4cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you want to run the trainer run the code below ( it takes about 12 minutes )\n",
    "# Its as easy as runnning trainer.train()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# If you want to evaluate the trainer run the code below\n",
    "\n",
    "trainer.evaluate(test_ds)\n",
    "predictions = trainer.predict(test_ds)\n",
    "output = np.argmax(predictions.predictions,1)\n",
    "sns.heatmap(confusion_matrix(test.label.values,output),annot =confusion_matrix(test.label.values,output) )#,labels = ['1','-1','0']\n",
    "\n",
    "del predictions #--> We delete the predictions as we don't want to occupy too much gpu space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbfe5c9-5e8d-44bc-a18b-9f63d530b85e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# deleting all our objects to save GPU space\n",
    "del model\n",
    "del trainer\n",
    "del train_ds\n",
    "del val_ds\n",
    "del test_ds\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb83862-35c5-4592-8f3c-f24e3045723c",
   "metadata": {},
   "source": [
    "## Defining the trainning arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad063e57-d68f-4a52-9221-1fdd253950bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try with finbert \n",
    "#  define tokenizer & model --> this is just a change from the previous code above\n",
    "finbert_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "# turn the configuration for a 3 sentiment classification task\n",
    "config = AutoConfig.from_pretrained(\"ProsusAI/finbert\")\n",
    "config.num_labels = 2\n",
    "\n",
    "finbert_model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "# redefine our datasets as wechanged the tokenizer \n",
    "\n",
    "MAX_LEN = 128\n",
    "train_ds = BERTTutorialDataset(\n",
    "    input_data=train['sentence'].to_numpy(),\n",
    "        sentiment_targets=train['label'].to_numpy(),\n",
    "        tokenizer=finbert_tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "val_ds = BERTTutorialDataset(\n",
    "    input_data=val['sentence'].to_numpy(),\n",
    "        sentiment_targets=val['label'].to_numpy(),\n",
    "        tokenizer=finbert_tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "\n",
    "test_ds = BERTTutorialDataset(\n",
    "    input_data=test['sentence'].to_numpy(),\n",
    "        sentiment_targets=test['label'].to_numpy(),\n",
    "        tokenizer=finbert_tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50401e03-6502-4056-9478-9870c0dc31cf",
   "metadata": {},
   "source": [
    "### Lauching the training preocess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4368c340-094d-4460-b7a4-3de3c8ae1613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define trainning arguments \n",
    "training_args = TrainingArguments('FINBERT_TUTORIAL_MODEL', overwrite_output_dir=True, evaluation_strategy=\"steps\", \n",
    "                                  num_train_epochs=50, weight_decay=1e-8,learning_rate=1e-5,\n",
    "                                  eval_steps=50,metric_for_best_model='accuracy',\n",
    "                                 per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "                                 load_best_model_at_end = True, save_total_limit=10, save_steps=50,no_cuda=False,\n",
    "                             fp16=True,gradient_accumulation_steps=4)\n",
    "trainer = Trainer(\n",
    "    model =finbert_model, args=training_args, train_dataset=train_ds, eval_dataset=val_ds,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=5)], compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bb6407-42b6-465e-b15e-603f7695b308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you want to run the trainer run the code below ( it takes about 10 minutes )\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# If you want to evaluate the trainer run the code below\n",
    "\n",
    "trainer.evaluate(test_ds)\n",
    "predictions = trainer.predict(test_ds)\n",
    "output = np.argmax(predictions.predictions,1)\n",
    "sns.heatmap(confusion_matrix(test.label.values,output),annot =confusion_matrix(test.label.values,output))\n",
    "\n",
    "del predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb885a34-57f9-48bc-98d4-ddc1c8b7efd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b097ca-9411-4721-9d37-157c05616bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting all our objects to save GPU space\n",
    "del finbert_model\n",
    "del trainer\n",
    "del train_ds\n",
    "del val_ds\n",
    "del test_ds\n",
    "del finbert_tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81bd1d3-d889-44e2-83a4-a465b0dfb443",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Masked Language Modelling code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9d861-f228-4161-bfab-c51bdcdc49b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will not be running the code below as it would take a long time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c2200-7216-4817-ac16-d2edc0b0862d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "```python\n",
    "# Import the model & tokenizer\n",
    "\n",
    "model_to_pretrain = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "tokenizer_for_pretraining = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# tokenize the inputs of the text \n",
    "inputs_for_pretraining = tokenizer_for_pretraining(df.sentence.tolist(), return_tensors='pt', max_length=32, truncation=True, padding='max_length')\n",
    "inputs_for_pretraining['labels'] = inputs.input_ids.detach().clone()\n",
    "\n",
    "# create random array of floats with equal dimensions to input_ids tensor\n",
    "random_mask = torch.rand(inputs_for_pretraining.input_ids.shape)\n",
    "\n",
    "# create mask array --> we hide 15% of the inputs for the masked language modelling task  \n",
    "mask_arr = (random_mask < 0.15) * (inputs_for_pretraining.input_ids != 101) * \\\n",
    "           (inputs_for_pretraining.input_ids != 102) * (inputs_for_pretraining.input_ids != 0)\n",
    "\n",
    "selection = []\n",
    "\n",
    "for i in range(inputs_for_pretraining.input_ids.shape[0]):\n",
    "    selection.append(\n",
    "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    )\n",
    "for i in range(inputs_for_pretraining.input_ids.shape[0]):\n",
    "    inputs_for_pretraining.input_ids[i, selection[i]] = 103\n",
    "    \n",
    "    \n",
    "class TUTORIALDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    This is also a Dataset class as the Dataset class before \n",
    "    \"\"\"\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n",
    "    \n",
    "dataset = TUTORIALDataset(inputs_for_pretraining)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# Move model to device\n",
    "model_to_pretrain.to(device)\n",
    "# launch model training\n",
    "model_to_pretrain.train()\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='out',\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=2 #only 2 train epochs --> this is a toy example, running a proper script would take upwards of 10 hours ! \n",
    ")\n",
    "\n",
    "MLM_trainer = Trainer(\n",
    "    model=model_to_pretrain,\n",
    "    args=args,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "MLM_trainer.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423bc5b6-81fd-4b69-ad92-60f6626fdbb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe53179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers_interpret import SequenceClassificationExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49975436-7777-4aff-8c59-df929efd047b",
   "metadata": {},
   "source": [
    "To visualise which words in each phrase are the most important for the prediction we will use the python package transformers_interpret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd84b54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fin_model_name = \"ProsusAI/finbert\"\n",
    "model_name = \"textattack/bert-base-uncased-SST-2\"\n",
    "\n",
    "\n",
    "fin_model = AutoModelForSequenceClassification.from_pretrained(fin_model_name)\n",
    "fin_tokenizer = AutoTokenizer.from_pretrained(fin_model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be06563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With both the model and tokenizer initialized we are now able to get explanations on an example text.\n",
    "cls_explainer = SequenceClassificationExplainer(model,\n",
    "                                                tokenizer)\n",
    "\n",
    "fin_cls_explainer = SequenceClassificationExplainer(fin_model,\n",
    "                                                    fin_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9979f669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff99d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_attributions = cls_explainer(\"Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&D and marketing\")\n",
    "word_attributions = fin_cls_explainer(\"Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&D and marketing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf21c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_explainer.predicted_class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6dda98",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_vis = cls_explainer.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050d7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_bert_vis = fin_cls_explainer.visualize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
